{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e43194-2035-4170-b2b3-e6b85004f5d1",
   "metadata": {
    "id": "d0e43194-2035-4170-b2b3-e6b85004f5d1"
   },
   "source": [
    "# Predicting brain activity from word embeddings during natural language comprehension\n",
    "This tutorial introduces a typical **enc**oding framework for mapping **ling**uistic embeddings onto human brain activity during natural language comprehension. The tutorial includes worked examples for both fMRI and ECoG datasets collected while subjects listened to naturalistic spoken narratives. Two types of word embeddings are obtained based on the stimulus transcripts: static word embeddings from word2vec and contextual word embeddings from GPT-2. Encoding models are estimated using banded ridge regression—this allows us to predict brain activity from word embeddings for left-out segments of data.\n",
    "\n",
    "*Acknowledgments:* This tutorial draws heavily on work by co-author Zaid Zada (e.g. code from [Zada et al., 2023](https://doi.org/10.1101/2023.06.27.546708)) as well as Gallant Lab's [voxelwise modeling tutorials](https://gallantlab.org/voxelwise_tutorials/index.html) ([Dupré La Tour et al., 2022](https://doi.org/10.1016/j.neuroimage.2022.119728)).\n",
    "\n",
    "---\n",
    "\n",
    "First, we'll install the necessary packages for this tutorial. After that, we'll import some general-purpose Python packages. Typically, you should import all of your packages at the beginning of a script; here, we'll import packages we need as we go. We'll also download some fMRI data, ECoG data, and the accompanying transcripts from Zenodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "HSOiNVwDgpxb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HSOiNVwDgpxb",
    "outputId": "664ea703-d4c7-48a0-dfee-a27758342aa5"
   },
   "outputs": [],
   "source": [
    "# Install packages not already in Colab\n",
    "# !pip install transformers accelerate\n",
    "# !pip install himalaya voxelwise_tutorials\n",
    "# !pip install nilearn surfplot neuromaps mne\n",
    "# !pip install jupyter_bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4YnDRVnBr6VQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4YnDRVnBr6VQ",
    "outputId": "b03059db-4119-468c-d49d-c542fb6cd119"
   },
   "outputs": [],
   "source": [
    "# # Some black magic to keep surfplot / VTK from crashing Colab kernel\n",
    "# # https://github.com/pyvista/pyvista/issues/313#issuecomment-512527853\n",
    "# !apt-get install -qq xvfb\n",
    "# !pip install panel\n",
    "\n",
    "# import os\n",
    "# os.system('/usr/bin/Xvfb :99 -screen 0 1024x768x24 &')\n",
    "# os.environ['DISPLAY'] = ':99'\n",
    "\n",
    "# import panel as pn\n",
    "# pn.extension('vtk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18750695-0b3a-4000-83b1-73727023dca4",
   "metadata": {
    "id": "18750695-0b3a-4000-83b1-73727023dca4"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import nilearn\n",
    "import transformers\n",
    "import accelerate\n",
    "import himalaya\n",
    "import surfplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mHEcU7aiIvNk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHEcU7aiIvNk",
    "outputId": "d807fdde-a8e5-4752-9357-d8441c0a393d"
   },
   "outputs": [],
   "source": [
    "# Download data from Zenodo if we don't already have it\n",
    "if not exists('encling-data.tgz'):\n",
    "    !curl -L https://zenodo.org/records/8216229/files/encling-data.tgz -o encling-data.tgz\n",
    "    !tar -xvzf encling-data.tgz\n",
    "    !mv encling-data/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jmRhBf5dJLIZ",
   "metadata": {
    "id": "jmRhBf5dJLIZ"
   },
   "source": [
    "## Natural language comprehension fMRI dataset\n",
    "In our first example, we'll use fMRI data collected for a single subject listening to a spoken story called \"[I Knew You Were Black](https://themoth.org/stories/i-knew-you-were-black)\" by Carol Daniel. These data are part of the publicly available [Narratives](https://github.com/snastase/narratives) collection ([Nastase et al., 2019](https://openneuro.org/datasets/ds002345)). This dataset has been preprocessed using [fMRIPrep](https://fmriprep.org/en/stable/) with confound regression in [AFNI](https://afni.nimh.nih.gov/). The functional data have been spatially normalized to a template in MNI space. To reduce computational demands, we compute parcel-wise ISCs using a cortical parcellation containing 400 parcels from [Schaefer and colleages (2018)](https://doi.org/10.1093/cercor/bhx179)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "D_yr9tnOJDsG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "D_yr9tnOJDsG",
    "outputId": "3901f8ae-4634-46c9-fe70-c61d0fc0804c"
   },
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "\n",
    "# Preprocessed fMRI data from Narratives\n",
    "func_fn = ('sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz')\n",
    "\n",
    "# Fetch Schaefer atlas with 400 parcels and 17 Yeo networks\n",
    "n_parcels = 400\n",
    "atlas = fetch_atlas_schaefer_2018(n_rois=n_parcels, yeo_networks=17, resolution_mm=2)\n",
    "\n",
    "# Initialize labels masker with atlas parcels\n",
    "masker = NiftiLabelsMasker(atlas.maps, labels=atlas.labels)\n",
    "\n",
    "# Fit masker to extract mean time series for parcels\n",
    "func_parcels = masker.fit_transform(func_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hub_A5CqJWu0",
   "metadata": {
    "id": "Hub_A5CqJWu0"
   },
   "source": [
    "The `NiftiLabelsMasker` provides an easy way to extract the mean BOLD time series from each parcel in the atlas. We'll plot the time series from an example parcel in anterior superior temporal cortex. To transform parcel-level data back into a brain image, we'll use the `masker`'s `.inverse_transform()` method; this will allow us to visualize the location of the example parcel on the brain. We'll plot this parcel using both [`nilearn`](https://nilearn.github.io/dev/index.html) and [`surfplot`](https://surfplot.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fKJZWHR8JD4U",
   "metadata": {
    "id": "fKJZWHR8JD4U"
   },
   "outputs": [],
   "source": [
    "# # Plot the time series for an example parcel\n",
    "# from nilearn.plotting import plot_stat_map\n",
    "\n",
    "# example_parcel = 195\n",
    "# func_parcel = func_parcels[:, example_parcel]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 2))\n",
    "# ax.plot(func_parcel)\n",
    "# ax.set(xlabel='TRs', ylabel='activity', xlim=(0, len(func_parcel)))\n",
    "# sns.despine()\n",
    "\n",
    "# # Plot parcel on MNI atlas\n",
    "# parcels_label = np.zeros(func_parcels.shape[1])\n",
    "# parcels_label[example_parcel] = 1\n",
    "\n",
    "# # Invert masker transform to project onto brain\n",
    "# parcel_img = masker.inverse_transform(parcels_label)\n",
    "# plot_stat_map(parcel_img, cmap='Blues');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "CdZ43AWLI_UD",
   "metadata": {
    "id": "CdZ43AWLI_UD"
   },
   "outputs": [],
   "source": [
    "# from surfplot import Plot\n",
    "# from neuromaps.datasets import fetch_fslr\n",
    "# from neuromaps.transforms import mni152_to_fslr\n",
    "\n",
    "# # Fetch fsLR surfaces from neuromaps\n",
    "# surfaces = fetch_fslr()\n",
    "# lh, rh = surfaces['inflated']\n",
    "# sulc_lh, sulc_rh = surfaces['sulc']\n",
    "\n",
    "# # Convert volumetric MNI data to fsLR surface\n",
    "# gii_lh, gii_rh = mni152_to_fslr(parcel_img, method='nearest')\n",
    "\n",
    "# # Plot example ROI on surface\n",
    "# p = Plot(surf_lh=lh, surf_rh=rh, brightness=.7)\n",
    "# p.add_layer({'left': gii_lh, 'right': gii_rh}, cmap='Blues', color_range=(0, 1))\n",
    "# cbar_kws = dict(location='right', draw_border=False, aspect=10,\n",
    "#                 shrink=.2, decimals=0, pad=0, n_ticks=2)\n",
    "# fig = p.build(cbar_kws=cbar_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad734fb9-6bf7-435c-8aa4-860098707880",
   "metadata": {
    "id": "ad734fb9-6bf7-435c-8aa4-860098707880"
   },
   "source": [
    "## Extracting word embeddings\n",
    "In the following sections, we extract two types of vectors—called \"word embeddings\"—capturing the meaning of the words in our transcript. In both cases, words are encoded as vectors of continuous numeric values in a high-dimensional embedding space where each dimension corresponds to an internal feature of the model. Words that are similar to each other are located nearing to each other in this embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064ce37-5a0f-470a-a9cc-ba6462b6c2f1",
   "metadata": {
    "id": "9064ce37-5a0f-470a-a9cc-ba6462b6c2f1"
   },
   "source": [
    "### Static word embeddings\n",
    "In the first case, we'll retrieve static, noncontextual word embeddings from a pre-trained model called word2vec ([Mikolov et al., 2013](https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html)). This word2vec model was (pre)trained the Google News corpus containing approximately 100 billion words. We use a 300-dimensional word2vec model; that is, the hidden layer contains 300 units, resulting in 300-long word vectors. The word2vec embeddings are considered static embeddings because they capture the global meaning of a given word, and each occurrence of a given word receives the same embedding regardless of the surrounding context. The model is about 2 GB in size, so it may take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d155f4ef-b39d-4f5a-9a1b-aed762903425",
   "metadata": {
    "id": "d155f4ef-b39d-4f5a-9a1b-aed762903425"
   },
   "outputs": [],
   "source": [
    "# import gensim.downloader\n",
    "\n",
    "# # Download 300-dimensional word2vec embeddings\n",
    "# model_name = 'word2vec-google-news-300'\n",
    "# n_features = 300\n",
    "\n",
    "# model = gensim.downloader.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vSEqYm8OJ0Wz",
   "metadata": {
    "id": "vSEqYm8OJ0Wz"
   },
   "source": [
    "For every word in our transcript, we'll retrieve the corresponding embedding. We'll ignore any words that are not found in the word2vec vocabulary. We'll add these embeddings into our transcript and resave the transcript for easier loading later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5bd2f06-68a2-4e5e-90cc-2362ff9af04a",
   "metadata": {
    "id": "d5bd2f06-68a2-4e5e-90cc-2362ff9af04a"
   },
   "outputs": [],
   "source": [
    "# # Load in transcript CSV file\n",
    "# transcript_f = 'black_transcript.csv'\n",
    "# transcript_w2v = pd.read_csv(transcript_f)\n",
    "\n",
    "# # Convert words to lowercase\n",
    "# transcript_w2v['word'] = transcript_w2v.word.str.lower()\n",
    "\n",
    "# # Function to extract embeddings if available\n",
    "# def get_vector(word):\n",
    "#     if word in model.key_to_index:\n",
    "#         return model.get_vector(word, norm=True).astype(np.float32)\n",
    "#     return np.nan\n",
    "\n",
    "# # Extract embedding for each word\n",
    "# transcript_w2v['embedding'] = transcript_w2v.word.apply(get_vector)\n",
    "# transcript_w2v = transcript_w2v.astype({'onset': 'float32', 'offset': 'float32'}, copy=False)\n",
    "\n",
    "# # Print out words not found in vocabulary\n",
    "# print(f'{(transcript_w2v.embedding.isna()).sum()} words not found:')\n",
    "# print(transcript_w2v.word[transcript_w2v.embedding.isna()].value_counts())\n",
    "\n",
    "# # Save transcript with embeddings using pickle\n",
    "# with open('black_w2v.pkl', 'wb') as f:\n",
    "#     pickle.dump(transcript_w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c77dae6-5bd1-4667-b287-0fb1752df166",
   "metadata": {
    "id": "5c77dae6-5bd1-4667-b287-0fb1752df166"
   },
   "outputs": [],
   "source": [
    "# # Reload transcript with embeddings if already generated\n",
    "# transcript_f = 'black_w2v.pkl'\n",
    "# if exists(transcript_f):\n",
    "#     with open(transcript_f, 'rb') as f:\n",
    "#         transcript_w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec25e9-9320-4c1e-9919-69a96b06ab8c",
   "metadata": {
    "id": "e7ec25e9-9320-4c1e-9919-69a96b06ab8c"
   },
   "source": [
    "### Contextual word embeddings\n",
    "In the second case, we'll extract contextual word embeddings from an autoregressive (or \"causal\") large language model (LLM) called GPT-2 ([Radford et al., 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). GPT-2 relies on the Transformer architecture to sculpt the embedding of a given word based on the preceding context. The model is composed of a repeated circuit motif—called the \"attention head\"—by which the model can \"attend\" to previous words in the context window when determining the meaning of the current word. This GPT-2 implementation is composed of 12 layers, each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 768 features and the context window includes the preceding 1024 tokens. Note that certain words will be broken up into multiple tokens; we'll need to use GPT-2's \"tokenizer\" to convert words into the appropriate tokens. GPT-2 has been (pre)trained on large corpora of text according to a simple self-supervised objective function: predict the next word based on the prior context. First, we'll initialize the tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "687a1872-ab77-4d30-a374-062e761c2f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626,
     "referenced_widgets": [
      "2f0d1b94e5d84aa68df51ce515225552",
      "76f78665bd60451194a2b88a770ce22f",
      "7b29474099f84e2ba3ea09c7982130cb",
      "5a37e874bb3c44a29d817f73e3603c38",
      "633fd0288ae943bdbb38b4656de761f9",
      "8e6fdc51719b4930bd6859cb620d7d16",
      "2f9f20efd76d4cb4a7c2e4e445da0c92",
      "7c111bafd3ba4d8583602742cfa43f1b",
      "280ecb113a03409d94b01e741558a3d9",
      "d0445857280f4b0a92d07ced9a049f7b",
      "fc926f5508a84096ae8272528f6a8b17",
      "d1ab9c3c61e440f599b9f40df06ccf80",
      "de48e5f069104ce0ae05f4080a1bca9a",
      "d324ed87f5474cf989a0cd395c36f06b",
      "ff70ced390954ddea604a36f609b7efd",
      "29dedfdd33cb48138ed8372995b281ce",
      "0dad9254f0004d7d82f99187638d222d",
      "b18bbf6ada7441178a218103e1201da9",
      "548287f046294c3fbb9c11ad2d9dcf77",
      "f44352668642421abb168d16d3baab40",
      "6f0c94e40b1d496c9f240f78cf3d000e",
      "b19ae96fd98b42e4a81df91063c65cbc",
      "90553677f945480b8e388d9d0ff655c8",
      "802bb91c1bab4935a9e0e626e69c1c23",
      "1458bca0a9384eb4bff0ec04a9aa776b",
      "a85ef175f2614b92abe824831dacdfe7",
      "df64d8dffa8a424dba98cb5ba221f87f",
      "cbf77b1b9ca145d0b8fa200e62588b14",
      "3591ab2035df4b9dbfa1691dfce82a12",
      "178ca28827c84bb0a13f9ceb680aa0d1",
      "6fa1483ce6014d8eb78a4ae03fa9e8e2",
      "846c26d4527f4be49c8479f5d20ec664",
      "634141202d4f46edabedb85b1a83c421",
      "4baf36cab4bb4e53b243d52d69f27bf9",
      "af638484af0b4c5abf24f9b11dff0539",
      "6fb88daec9824960a6b17b30a8bbd45a",
      "436f762787b546ee90ed3e0b31e2cf20",
      "06c813b116dc40fbaf85c528832407fc",
      "86cec5c955304fe5a04457c99f23e85c",
      "a67d3d8cb0f445ef8712a438c15321ea",
      "2e30d7afdfdf4d66872eaddcf4872cf4",
      "8179a84a39a8494db5ceff251474863a",
      "582505944b2d4f59a375eb6be475250d",
      "1426b1cfe4e84475b1c25d83e76eeacf",
      "80c9606f7ad042648c265b893ff6d04f",
      "87f0c7da96cf46aa9cdddc2021e99fd6",
      "8706611590594a32a2d9da8d7b3cff71",
      "1c8bb4a103474401bf5003513ac410ef",
      "2ec8711f757f48ef88e876728099f2e7",
      "25bb9bf7027f48be8fba443662145078",
      "b522502e622c437b829f57ef7cea828c",
      "108e33f92dbe457d8487de2b2186db78",
      "cfaa3e2befab49a8b6f8ac8d315dc3a3",
      "7ac1c8c55f0045c19d29123dc518526e",
      "849ffe145e8642c09e2f07db9e40e146",
      "70b5ea4a8e8f40c1a77587af4513abb2",
      "6534373e815642788319153b42cfdb24",
      "7788ae55e7664c029e5a930d395d99de",
      "a41f25eb0c4d41c79940d62c0cbaf8dc",
      "13af58c6b20649fe922bb0c6223c2f0f",
      "72d80169b45c4f178ec667d507e94618",
      "21318a73cce6402da95aba96df138c80",
      "7e85b53d80c845cca020ab3383dce282",
      "828d88411bcc497a9a011151b11dfa93",
      "b73dda287da74163a7beee26c22f4652",
      "4046a45c3d064ef0a7892841e7f8116c",
      "ba03cc618c364a3fb06ed6acbc1b7c50",
      "d5813bfcb5ab4912844646a5a66318c6",
      "d5bf4fe82eb3409db366b738cd35dc65",
      "c561e37685e442368cbc0e282d950edd",
      "08b47210768c4c248cbb9c74394253e7",
      "062ce0cefdc3424ea03131ef193071d1",
      "266d23b0dbf94c6c85b205e9cd7aba0b",
      "601440f5a80f41cbac3c1c0ff73beef7",
      "3de6628df0644b82a3d8bb548bc267b3",
      "7dcc21f2ee9c42c9af91a9a7c5a47f33",
      "e48421204bbd4b39ab8da5272f5b6efb"
     ]
    },
    "id": "687a1872-ab77-4d30-a374-062e761c2f40",
    "outputId": "3b61cb18-3be3-4655-d725-8a8dd6a6394b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/ri4541/miniconda3/envs/encling/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "n_features = 768\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print out model architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b-s6xGSWKNep",
   "metadata": {
    "id": "b-s6xGSWKNep"
   },
   "source": [
    "Running the model on our transcript will be faster if GPUs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cfe9b43-bbde-49ba-a971-3482af83cb66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cfe9b43-bbde-49ba-a971-3482af83cb66",
    "outputId": "4d883288-1b6e-48a3-9177-385d48458cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get device for running model\n",
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RMPjbvTfKWPi",
   "metadata": {
    "id": "RMPjbvTfKWPi"
   },
   "source": [
    "Now, we reload our original transcript. We'll run the tokenizer to convert the words into subword tokens for input to GPT-2. We need to keep track of the word indices when we run the tokenizer; we'll get an embedding for each token, but we'll want to recombine (i.e. average) embeddings for words split into multiple tokens. These tokens are supplied to GPT-2 as integer IDs corresponding to words the GPT-2 vocabulary (which contains approximately 50,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22b1802-92c6-4e91-b474-e2b175c88515",
   "metadata": {
    "id": "f22b1802-92c6-4e91-b474-e2b175c88515"
   },
   "outputs": [],
   "source": [
    "# Reload in transcript CSV file\n",
    "transcript_f = 'black_transcript.csv'\n",
    "transcript_gpt2 = pd.read_csv(transcript_f)\n",
    "\n",
    "# Insert explicit index column for reference\n",
    "transcript_gpt2.insert(0, 'word_index', transcript_gpt2.index.values)\n",
    "\n",
    "# Tokenize words into lists of tokens\n",
    "transcript_gpt2['token'] = transcript_gpt2.word.apply(tokenizer.tokenize)\n",
    "\n",
    "# \"Explode\" lists of token subwords into long format\n",
    "transcript_gpt2 = transcript_gpt2.explode('token', ignore_index=True)\n",
    "\n",
    "# Convert tokens to token IDs for input to model\n",
    "transcript_gpt2['token_id'] = transcript_gpt2.token.apply(tokenizer.convert_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amqCUA3QKdHQ",
   "metadata": {
    "id": "amqCUA3QKdHQ"
   },
   "source": [
    "Since our transcript contains more tokens than can be contained in GPT-2's context window, we'll aggregate the tokens into multiple `samples` matching the width of the context window (1024 tokens) and spanning the full length of the transcript. On some systems, we may be able to use the `accelerate` package to speed up model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d76422-a21b-4679-9a6f-b3cf777c0d7e",
   "metadata": {
    "id": "10d76422-a21b-4679-9a6f-b3cf777c0d7e"
   },
   "outputs": [],
   "source": [
    "# Convert all token IDs into list\n",
    "token_ids = transcript_gpt2.token_id.tolist()\n",
    "\n",
    "# Extract context window width for model\n",
    "max_len = tokenizer.model_max_length\n",
    "\n",
    "# Compile into lists of tokens within each context window\n",
    "samples = []\n",
    "token_ids = torch.tensor(transcript_gpt2.token_id.tolist(), dtype=torch.long)\n",
    "samples.append(token_ids[0:max_len])\n",
    "for i in range(max_len+1, len(token_ids)+1):\n",
    "    samples.append(token_ids[i-max_len:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169bb016-0cb2-4269-b5ab-ff22f4323c74",
   "metadata": {
    "id": "169bb016-0cb2-4269-b5ab-ff22f4323c74"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize accelerator and free memory\n",
    "accelerator = Accelerator()\n",
    "accelerator.free_memory()\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oQuWINgtKlze",
   "metadata": {
    "id": "oQuWINgtKlze"
   },
   "source": [
    "Finally, we'll use a PyTorch `DataLoader` to supply token IDs to the model in batches and extract the embeddings. In addition to the embeddings, we'll also extract several other features of potential interest from the model. As GPT-2 proceeds through the text, it generates a probability distribution (the `logits` extracted below) across all words in the vocabulary with the goal of correctly predicting the next word. We can use this probability distribution to derive other features of the model's internal computations. We'll extract the following features from GPT-2:\n",
    "\n",
    "* `embeddings`: the 768-dimensional contextual embedding capturing the meaning of the current word\n",
    "* `top_guesses`: the highest probability word GPT-2 predicts for the current word\n",
    "* `ranks`: the rank of the correct word given probabilities across the vocabulary\n",
    "* `true_probs`: the probability at which GPT-2 predicted the current word\n",
    "* `entropies`: how the uncertain GPT-2 was about the current word\n",
    "  * low entropy indicates that the probability distribution was \"focused\" on certain words\n",
    "  * high entropy indicates the  probability distribution was more uniform/dispersed across words\n",
    "\n",
    "*Note:* This upcoming cell may take some time to run if you're relying on CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fc1c7-a1fc-4506-b335-4e1c2699e46e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "13f62db2e3874d668e028c9a7d8e16d7",
      "ecd3db9d03c6454b97b66c38152d4801",
      "ffac94d3df9b4404b99904eebd430a63",
      "64e9caad394a427bb24067f43528f259",
      "896a1c434f6c46b6b8388837cf4a88bb",
      "a95dcd67665a4a27979e5e8f2eb796a6",
      "192a08da88c64b0db8f2b2f95fc7c50e",
      "b9513445421b4d99a44006434393731d",
      "8c366283951a4fd39dd7faba7772d12d",
      "1d000c8ca15f4d3b9fca05712d0fc006",
      "b8bc017558164f4181616bd99089d438"
     ]
    },
    "id": "ff3fc1c7-a1fc-4506-b335-4e1c2699e46e",
    "outputId": "4a536174-06bc-466d-829d-a1ff92d4ebf1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34561e532e344b1ca1240960d5f8d2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set a batch size for the data loader\n",
    "batch_size = 4\n",
    "\n",
    "# Extract a late-intermediate layer from GPT-2\n",
    "layer = 8\n",
    "\n",
    "# Extract embeddings and other model features\n",
    "embeddings = []\n",
    "top_guesses = []\n",
    "ranks = []\n",
    "true_probs = []\n",
    "entropies = []\n",
    "with torch.no_grad():\n",
    "    data_loader = torch.utils.data.DataLoader(samples, batch_size=batch_size,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    # Loop through samples and extract embeddings\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        output = model(batch.to(device), output_hidden_states=True)\n",
    "        logits = output.logits  # torch.Size([2, 1024, 50257])\n",
    "        states = output.hidden_states[layer]\n",
    "\n",
    "        # Extract all embeddings/features for first context window\n",
    "        if i == 0:\n",
    "            true_ids = batch[0, :]\n",
    "            brange = list(range(len(true_ids)-1))\n",
    "            logits_order = logits[0].argsort(descending=True, dim=-1)\n",
    "            batch_top_guesses = logits_order[:-1, 0]\n",
    "            batch_ranks = torch.eq(logits_order[:-1],\n",
    "                                   true_ids.reshape(-1,1)[1:].to(device)).nonzero()[:, 1]\n",
    "            batch_probs = logits[0, :-1].softmax(-1)\n",
    "            batch_true_probs = batch_probs[brange, true_ids[1:]]\n",
    "            batch_entropy = torch.distributions.Categorical(probs=batch_probs).entropy()\n",
    "            batch_embeddings = states[0]\n",
    "\n",
    "            top_guesses.append(batch_top_guesses.numpy(force=True))\n",
    "            ranks.append(batch_ranks.numpy(force=True))\n",
    "            true_probs.append(batch_true_probs.numpy(force=True))\n",
    "            entropies.append(batch_entropy.numpy(force=True))\n",
    "            embeddings.append(batch_embeddings.numpy(force=True))\n",
    "\n",
    "            # Reset if there are samples remaining in this batch\n",
    "            if batch.size(0) == 1:\n",
    "                continue\n",
    "            logits = logits[1:]\n",
    "            states = states[1:]\n",
    "            batch = batch[1:]\n",
    "\n",
    "        # Extract embeddings/features for last word in subsequent windows\n",
    "        true_ids = batch[:, -1]\n",
    "        brange = list(range(len(true_ids)))\n",
    "        logits_order = logits[:, -2, :].argsort(descending=True)  # batch x vocab_size\n",
    "        batch_top_guesses = logits_order[:, 0]\n",
    "        batch_ranks = torch.eq(logits_order, true_ids.reshape(-1,1).to(device)).nonzero()[:, 1]\n",
    "        batch_probs = torch.softmax(logits[:, -2, :], dim=-1)\n",
    "        batch_true_probs = batch_probs[brange, true_ids]\n",
    "        batch_entropy = torch.distributions.Categorical(probs=batch_probs).entropy()\n",
    "        batch_embeddings = states[:, -1, :]\n",
    "\n",
    "        top_guesses.append(batch_top_guesses.numpy(force=True))\n",
    "        ranks.append(batch_ranks.numpy(force=True))\n",
    "        true_probs.append(batch_true_probs.numpy(force=True))\n",
    "        entropies.append(batch_entropy.numpy(force=True))\n",
    "        embeddings.append(batch_embeddings.numpy(force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QO05oqZkLAau",
   "metadata": {
    "id": "QO05oqZkLAau"
   },
   "source": [
    "We'll recompile the features we extracted from GPT-2 into our transcript and save it for easier loading later on. We'll also summarize how accurately GPT-2 was able to predict upcoming words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ea22f-3554-4c65-a808-c53ac32a3a9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e67ea22f-3554-4c65-a808-c53ac32a3a9e",
    "outputId": "d222ed58-1bb4-4320-d228-899e11f9aa2d"
   },
   "outputs": [],
   "source": [
    "# Compile outputs into transcript (logit derivatives must be shifted by 1)\n",
    "transcript_gpt2.loc[1:, 'rank'] = np.concatenate(ranks)\n",
    "transcript_gpt2.loc[1:, 'true_prob'] = np.concatenate(true_probs)\n",
    "transcript_gpt2.loc[1:, 'top_pred'] = np.concatenate(top_guesses)\n",
    "transcript_gpt2.loc[0, 'top_pred'] = tokenizer.bos_token_id\n",
    "transcript_gpt2.loc[1:, 'entropy'] = np.concatenate(entropies)\n",
    "transcript_gpt2['embedding'] = [e for e in np.vstack(embeddings)]\n",
    "\n",
    "# Reduce size of transcript\n",
    "transcript_gpt2 = transcript_gpt2.astype({'word_index': 'int32', 'onset': 'float32',\n",
    "                                      'offset': 'float32', 'token_id': 'int32',\n",
    "                                      'rank': 'float32', 'true_prob': 'float32',\n",
    "                                      'top_pred': 'int32', 'entropy': 'float32'}, copy=False)\n",
    "\n",
    "# Convert model's top predictions from token IDs to tokens\n",
    "transcript_gpt2['top_pred'] = transcript_gpt2.top_pred.apply(tokenizer.convert_ids_to_tokens)\n",
    "\n",
    "# Print out top-1 and top-10 word prediction accuracy\n",
    "print(f\"Top-1 accuracy: {(transcript_gpt2['rank'] == 0).mean():.3f}\")\n",
    "print(f\"Top-10 accuracy: {(transcript_gpt2['rank'] < 10).mean():.3f}\")\n",
    "\n",
    "# Save transcript with embeddings using pickle\n",
    "with open('black_gpt2.pkl', 'wb') as f:\n",
    "    pickle.dump(transcript_gpt2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74dfe65-8db4-4e80-b99a-c1feb147225b",
   "metadata": {
    "id": "a74dfe65-8db4-4e80-b99a-c1feb147225b"
   },
   "outputs": [],
   "source": [
    "# Reload transcript with embeddings if already generated\n",
    "transcript_f = 'black_gpt2.pkl'\n",
    "if exists(transcript_f):\n",
    "    with open(transcript_f, 'rb') as f:\n",
    "        transcript_gpt2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3gmhh7nLRWS",
   "metadata": {
    "id": "h3gmhh7nLRWS"
   },
   "source": [
    "## Estimating and evaluating encoding models\n",
    "In the following section, we'll use the embeddings extracted above to construct our encoding model. The goal of the encoding model is to evaluate how well we can predict brain activity from our representation of the stimulus in a given feature space ([Naselaris et al., 2011](https://doi.org/10.1016/j.neuroimage.2010.07.073)); in this case, our word embeddings encode linguistic content of the spoken story. We'll use linear regression to estimate a mapping from the word embeddings to the fMRI time series at each cortical parcel. Our model is very wide—containing hundreds or thousands of features or predictors (i.e. columns)—relative to the number of samples (i.e. TRs), so we are prone to overfitting. We'll rely on two fundamental principles of machine learning to mitigate the risks of overfitting. First, we'll introduce a *regularization* penalty (i.e. a particular kind of bias) into our regression equation to constrain how well the model (over)fits to the training data; ridge regression effectively \"squeezes\" the regression weights to reduce overfitting. Second, we will use *out-of-sample prediction* to evaluate the performance of our model; that is, we'll use cross-validation to train the model on subsets of the data and test the model on non-overlapping subsets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a12e5d-e035-4115-8d5d-5b180e92850d",
   "metadata": {
    "id": "85a12e5d-e035-4115-8d5d-5b180e92850d"
   },
   "source": [
    "### Construct predictor matrix\n",
    "First, we need to resample our embeddings to ensure we have only vector per TR; the number of samples in our targets `Y` (parcelwise BOLD time series) and our predictors `X` must match. In this study, the fMRI data were acquired with 1.5-second TRs. For TRs with multiple embeddings (e.g. multiple words or tokens), we'll average the embeddings; for TRs with no embeddings (e.g. moments of silence), we'll insert zero vectors matching the length of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c126348-9615-4da7-b331-33408e662fed",
   "metadata": {
    "id": "2c126348-9615-4da7-b331-33408e662fed"
   },
   "outputs": [],
   "source": [
    "# Function to average embeddings per TR\n",
    "def construct_predictors(transcript_df, n_features, stim_dur, tr=1.5):\n",
    "\n",
    "    # Find total number of TRs\n",
    "    stim_trs = np.ceil(stim_dur / tr)\n",
    "\n",
    "    # Add column to transcript with TR indices\n",
    "    transcript_df['TR'] = transcript_df.onset.divide(tr).apply(np.floor).apply(int)\n",
    "\n",
    "    # Compile the words within each TR\n",
    "    words_per_tr = transcript_df.groupby('TR')['word'].apply(list)\n",
    "\n",
    "    # Average the embeddings within each TR\n",
    "    embeddings_per_tr = transcript_df.groupby('TR')['embedding'].mean()\n",
    "\n",
    "    # Loop through TRs\n",
    "    words_trs = []\n",
    "    embeddings_trs = []\n",
    "    for t in np.arange(stim_trs):\n",
    "        if t in words_per_tr:\n",
    "            words_trs.append(words_per_tr[t])\n",
    "\n",
    "            # Fill in empty TRs with zero vectors\n",
    "            if embeddings_per_tr[t] is not np.nan:\n",
    "                embeddings_trs.append(embeddings_per_tr[t])\n",
    "            else:\n",
    "                embeddings_trs.append(np.zeros(n_features))\n",
    "        else:\n",
    "            words_trs.append([])\n",
    "            embeddings_trs.append(np.zeros(n_features))\n",
    "\n",
    "    embeddings = np.vstack(embeddings_trs)\n",
    "    return embeddings\n",
    "\n",
    "# word2vec embeddings are 300-dimensional\n",
    "# X_w2v = construct_predictors(transcript_w2v, 300, 800, tr=1.5)\n",
    "\n",
    "# GPT-2 embeddings are 768-dimensional\n",
    "X_gpt2 = construct_predictors(transcript_gpt2, 768, 800, tr=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ac8ff-0f21-417f-8f0e-b5bb69eba999",
   "metadata": {
    "id": "598ac8ff-0f21-417f-8f0e-b5bb69eba999"
   },
   "outputs": [],
   "source": [
    "# # Visualize word embedding predictors\n",
    "# from scipy.stats import zscore\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(9, 4), sharey=True)\n",
    "# axs[0].matshow(zscore(X_w2v, axis=0),\n",
    "#                cmap='binary_r', vmin=-3, vmax=3)\n",
    "# axs[0].set(xlabel='word2vec features', ylabel='TRs')\n",
    "# axs[1].matshow(zscore(X_gpt2, axis=0),\n",
    "#                cmap='binary_r', vmin=-3, vmax=3)\n",
    "# axs[1].set(xlabel='GPT-2 features');\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-86h91CQNOKU",
   "metadata": {
    "id": "-86h91CQNOKU"
   },
   "source": [
    "The fMRI data were acquired the 8 TRs of fixation both before and after the story stimulus. We'll trim these TRs off to ensure the fMRI data match the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3193e6c-60b2-4201-8b23-16bb8efb1381",
   "metadata": {
    "id": "d3193e6c-60b2-4201-8b23-16bb8efb1381"
   },
   "outputs": [],
   "source": [
    "# Trim fMRI data to match embeddings:\n",
    "start_trs = 8\n",
    "end_trs = 8\n",
    "\n",
    "# assert start_trs + X_w2v.shape[0] + end_trs == func_parcels.shape[0]\n",
    "\n",
    "Y_parcels = func_parcels[start_trs:-end_trs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b696635-a274-4ab5-8287-58cdd779f4f8",
   "metadata": {
    "id": "0b696635-a274-4ab5-8287-58cdd779f4f8"
   },
   "source": [
    "### Ridge regression\n",
    "Next, we'll use ridge regression to predict the activity at each parcel from the word embeddings. For this section, we'll consider each feature space (i.e. word2vec, GPT-2) in isolation. We'll use a split-half outer cross-validation scheme where we train the model on half of the story and test the model on the other half. To search for the best-performing regularization parameter $\\lambda$ (`alpha` in `scikit-learn` convention), we'll perform 5-fold inner cross-validation within each training set using `KernelRidgeCV`; this will select the best parameter setting from the inner cross-validation fold within the training half to predict the test half. Higher `alpha` values increase regularization and reduce overfitting. Within each cross-validation fold, we'll apply two transforms: `StandardScaler` will be used to mean-center or z-score each column of the predictor matrix within the training set, then apply that transformation to the training set; `Delayer` will horiztonally stack lagged versions our predictor matrix to account for the hemodynamic lag. We use `himalaya`'s `KernelRidgeCV` (rather than `RidgeCV`) because the multi-delayed version of the predictor matrix will be considerably wider than the number of fMRI samples. We'll combine these transforms and the estimator into a `Pipeline` that will run the whole analysis. This analysis qualitatively reproduces one of the core results from Huth and colleagues ([2016](https://doi.org/10.1038/nature17637))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb628d-02f5-44c4-858c-9e1cfc706783",
   "metadata": {
    "id": "13fb628d-02f5-44c4-858c-9e1cfc706783"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from sklearn.model_selection import KFold\n",
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Split-half outer and inner cross-validation\n",
    "outer_cv = KFold(n_splits=2)\n",
    "inner_cv = KFold(n_splits=5)\n",
    "\n",
    "# Mean-center each feature (columns of predictor matrix)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# Create delays at 3, 4.5, 6, 7.5 seconds (1.5 s TR)\n",
    "delayer = Delayer(delays=[2, 3, 4, 5])\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV\n",
    "alphas = np.logspace(1, 10, 10)\n",
    "ridge = KernelRidgeCV(alphas=alphas, cv=inner_cv)\n",
    "\n",
    "# Chain transfroms and estimator into pipeline\n",
    "pipeline = make_pipeline(scaler, delayer, ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8S2dlsB5NxQ4",
   "metadata": {
    "id": "8S2dlsB5NxQ4"
   },
   "source": [
    "We'll run ridge regression separately for each feature space separately. Run the encoding analysis on the `word2vec` embeddings first and have a look at the results, then re-run the whole pipeline with the `GPT-2` embeddings. In the following cell, we loop through the outer cross-validation folds, fit the pipeline within each fold, then generate predictions. We use the model weights estimated from the training data to predict the brain activity from word embeddings for the test data. The `himalaya` implementation makes this surprisingly fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a958e71-0415-4ef0-af78-0590b392d2d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a958e71-0415-4ef0-af78-0590b392d2d7",
    "outputId": "0b910c4d-6bbf-4e97-f423-844570ddcffc"
   },
   "outputs": [],
   "source": [
    "# Select embeddings from one of the models\n",
    "X = X_gpt2\n",
    "\n",
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "\n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X[train],\n",
    "                 Y_parcels[train])\n",
    "\n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X[test])\n",
    "    Y_predicted.append(predicted)\n",
    "\n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.vstack(Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d638c-2666-4149-a95a-8344a23b61ff",
   "metadata": {
    "id": "cb4d638c-2666-4149-a95a-8344a23b61ff"
   },
   "source": [
    "To evaluate the predictions of our model, we quantify the similarity between the predicted brain activity and the actual brain activity for the test data. Keeping with conventions in the literature, we use Pearson correlation to assess the match between predicted and actual brain activity. The `correlation_score` from `himalaya` computes the Pearson correlation between the actual and predicted test time series for each parcel. We'll create a simple histogram to visualize the correlation scores across all parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b09578-4323-4419-a4b0-6447a7898cfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "34b09578-4323-4419-a4b0-6447a7898cfc",
    "outputId": "3ea5030c-5749-4472-d862-f9bc0aeee82e"
   },
   "outputs": [],
   "source": [
    "from himalaya.scoring import correlation_score\n",
    "\n",
    "# Evaluate predictions: correlation between predicted and actual time series\n",
    "score_parcels = correlation_score(Y_parcels, Y_predicted)\n",
    "\n",
    "print(f\"Mean encoding performance: r = {np.mean(score_parcels):.3f}\")\n",
    "print(f\"Maximum encoding performance: r = {np.amax(score_parcels):.3f}\")\n",
    "\n",
    "# Plot a histogram of prediction performance values\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(score_parcels, stat='proportion', ax=ax)\n",
    "ax.set(xlabel='encoding performance (r)', ylabel='proportion of parcels');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_zoGmhSYOCLZ",
   "metadata": {
    "id": "_zoGmhSYOCLZ"
   },
   "source": [
    "We can also certain attributes of our fitted encoding model. For example, we may want to examine, the best alpha values selected from hyperparameter search, or we may want to recover the weight vectors estimated for a given training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6ce1b-128b-4ff0-b71c-50d65bb2beb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1d6ce1b-128b-4ff0-b71c-50d65bb2beb2",
    "outputId": "773bd88a-197d-4ea9-d6af-943f31570d0b"
   },
   "outputs": [],
   "source": [
    "# Introspect fitted pipeline model for alphas and weights\n",
    "ridge_fitted = pipeline['kernelridgecv']\n",
    "best_alphas = ridge_fitted.best_alphas_\n",
    "weights = ridge_fitted.get_primal_coef(ridge_fitted.X_fit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CszdIgfrZTe9",
   "metadata": {
    "id": "CszdIgfrZTe9"
   },
   "outputs": [],
   "source": [
    "# take sample text from the training data\n",
    "# use gpt2 to tokenize\n",
    "# use beam search to generate 5 continuations\n",
    "# use encoding model to predict bold activity for each continuation\n",
    "# rank predictions based on similarity to true bold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8q8hcI8vVx5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8q8hcI8vVx5c",
    "outputId": "e316966c-6212-4685-83af-f5602abeb66e"
   },
   "outputs": [],
   "source": [
    "transcript_gpt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KMgb2UNYaOKz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMgb2UNYaOKz",
    "outputId": "edbcc3c4-1f33-49b8-fa0e-9b739ee2d2e3"
   },
   "outputs": [],
   "source": [
    "# \"sample\" means the input text that we use as the starting point for prediction\n",
    "tr_start = 0  # TR to start the sample at\n",
    "n_trs_to_sample = 3  # duration (in TRs) of the sample\n",
    "n_trs_to_test = 3  # how much further (in TRs) past the sample do you want to predict?\n",
    "\n",
    "sample_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample].index  # token indices corresponding to sample\n",
    "test_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample+n_trs_to_test].index  # token indices corresponding to test\n",
    "new_token_idx = [x for x in test_idx if x not in sample_idx]  # the token indices that are in the test but not the sample\n",
    "\n",
    "sample_text = \" \".join(list(transcript_gpt2['word'][sample_idx.min():sample_idx.max()]))\n",
    "ground_truth = \" \".join(list(transcript_gpt2['word'][test_idx.min():test_idx.max()]))\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate continuations using nucleus sampling (https://huggingface.co/blog/how-to-generate)\n",
    "beam_outputs = model.generate(**inputs,\n",
    "                              num_return_sequences=5,\n",
    "                              max_new_tokens=len(new_token_idx),  # cheating because we know the number of tokens we want to predict (would be replaced by word rate model)\n",
    "                              do_sample=True,\n",
    "                              top_p=0.92,\n",
    "                              top_k=20,\n",
    "                              temperature=1.0)\n",
    "\n",
    "# Decode the generated sequences\n",
    "generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in beam_outputs]\n",
    "\n",
    "all_pred_bold = []\n",
    "for text in generated_texts:\n",
    "    # 1. Tokenize the generated text:\n",
    "    tokenized_text = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 2. Get embeddings from GPT-2:\n",
    "    with torch.no_grad():\n",
    "        output = model(tokenized_text, output_hidden_states=True)\n",
    "        embeddings = output.hidden_states[8][0].cpu().numpy()  # Assuming layer 8, adjust if necessary.\n",
    "        # print(embeddings.shape)\n",
    "\n",
    "    # 3. Predict BOLD activity using fitted encoding model\n",
    "    delay = 0\n",
    "    predicted_bold = embeddings @ weights[768*delay:768*(delay+1)]  # weights have 4 delay options\n",
    "    all_pred_bold.append(predicted_bold)\n",
    "\n",
    "# Rank predictions based on similarity to true BOLD data\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# index pred_bold at new tokens only; index Y_parcels at new TRs only\n",
    "similarities = [pearsonr(pred_bold[-1*len(new_token_idx):].mean(axis=0), Y_parcels[tr_start+n_trs_to_sample:tr_start+n_trs_to_sample+n_trs_to_test].mean(axis=0))[0] for pred_bold in all_pred_bold]\n",
    "\n",
    "sorted_indices = np.argsort(similarities)[::-1] # [::-1] reverses a list so it's in descending order of similarity\n",
    "\n",
    "print('Input: ', sample_text)\n",
    "print('True continuation:', '\\n', ground_truth, '\\n')\n",
    "\n",
    "# Print ranked continuations and their similarities\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"Rank #{i+1}, candidate #{idx+1}, similarity={similarities[idx]:.4f}:\")\n",
    "    print(generated_texts[idx] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epCPhBRFNzgf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epCPhBRFNzgf",
    "outputId": "f07a1622-fd53-4ffa-ece8-6df000a6d883",
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_candidates = 3\n",
    "\n",
    "# \"sample\" means the input text that we use as the starting point for prediction\n",
    "tr_start = 0  # TR to start the sample at\n",
    "n_trs_to_sample = 3  # duration (in TRs) of the sample\n",
    "n_trs_to_test = 3  # how much further (in TRs) past the sample do you want to predict?\n",
    "\n",
    "sample_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample].index  # token indices corresponding to sample\n",
    "test_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample+n_trs_to_test].index  # token indices corresponding to test\n",
    "new_token_idx = [x for x in test_idx if x not in sample_idx]  # the token indices that are in the test but not the sample\n",
    "\n",
    "sample_text = \" \".join(list(transcript_gpt2['word'][sample_idx.min():sample_idx.max()]))\n",
    "ground_truth = \" \".join(list(transcript_gpt2['word'][test_idx.min():test_idx.max()]))\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "try:\n",
    "  del sorted_generations\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for i in range(num_candidates):\n",
    "  print('\\n', 'Iteration', i)\n",
    "  try:\n",
    "    inputs = tokenizer(sorted_generations[i], return_tensors=\"pt\").to(device)\n",
    "    print('updating inputs')\n",
    "    # breakpoint()\n",
    "  except:\n",
    "    pass\n",
    "  # Generate continuations using nucleus sampling (https://huggingface.co/blog/how-to-generate)\n",
    "  print(inputs)\n",
    "  beam_outputs = model.generate(**inputs,\n",
    "                                num_return_sequences=num_candidates,\n",
    "                                max_new_tokens=len(new_token_idx),  # cheating because we know the number of tokens we want to predict (would be replaced by word rate model)\n",
    "                                do_sample=True,\n",
    "                                top_p=0.92,\n",
    "                                top_k=20,\n",
    "                                temperature=1.0,\n",
    "                                pad_token_id=50256)\n",
    "\n",
    "  # Decode the generated sequences\n",
    "  generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in beam_outputs]\n",
    "\n",
    "  all_pred_bold = []\n",
    "  for text in generated_texts:\n",
    "      # 1. Tokenize the generated text:\n",
    "      tokenized_text = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "      # 2. Get embeddings from GPT-2:\n",
    "      with torch.no_grad():\n",
    "          output = model(tokenized_text, output_hidden_states=True)\n",
    "          embeddings = output.hidden_states[8][0].cpu().numpy()  # layer 8\n",
    "          # print(embeddings.shape)\n",
    "\n",
    "      # 3. Predict BOLD activity using fitted encoding model\n",
    "      delay = 0\n",
    "      predicted_bold = embeddings @ weights[768*delay:768*(delay+1)]  # weights have 4 delay options\n",
    "      all_pred_bold.append(predicted_bold)\n",
    "\n",
    "  # Rank predictions based on similarity to true BOLD data\n",
    "  from scipy.stats import pearsonr\n",
    "\n",
    "  # index pred_bold at new tokens only; index Y_parcels at new TRs only\n",
    "  similarities = [pearsonr(pred_bold[-1*len(new_token_idx):].mean(axis=0),\n",
    "                           Y_parcels[tr_start+n_trs_to_sample:tr_start+n_trs_to_sample+n_trs_to_test].mean(axis=0))[0]\n",
    "                  for pred_bold in all_pred_bold]\n",
    "\n",
    "  sorted_indices = np.argsort(similarities)[::-1] # [::-1] reverses a list so it's in descending order of similarity\n",
    "\n",
    "  print('Input: ', tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "  # print('True continuation:', '\\n', ground_truth, '\\n')\n",
    "\n",
    "  # Print ranked continuations and their similarities\n",
    "  for i, idx in enumerate(sorted_indices):\n",
    "      print(f\"Rank #{i+1}, candidate #{idx+1}, similarity={similarities[idx]:.4f}:\")\n",
    "      print(generated_texts[idx] + '\\n')\n",
    "\n",
    "  sorted_generations = [generated_texts[i] for i in sorted_indices]\n",
    "  # print(len(sorted_generations))\n",
    "  # if len(sorted_generations) > num_candidates*5:\n",
    "  #     sorted_generations = sorted_generations[:num_candidates*5]  # prune the candidates post-sorting to prevent the number from growing too fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XvDQjsCIZWU3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvDQjsCIZWU3",
    "outputId": "a35f02fe-67df-4319-94e8-235d7df6eae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      " Iteration 0\n",
      "\n",
      " Iteration 1\n",
      "updating inputs\n",
      "\n",
      " Iteration 2\n",
      "updating inputs\n",
      "[[' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no magic involved. I', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was so much to see there']]\n",
      "[]\n",
      "\n",
      " Iteration 0\n",
      "updating inputs\n",
      "\n",
      " Iteration 1\n",
      "updating inputs\n",
      "\n",
      " Iteration 2\n",
      "updating inputs\n",
      "[[' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the school library and was hooked', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the library.\\n\\nMy', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the series, and they all']]\n",
      "[]\n",
      "\n",
      " Iteration 0\n",
      "updating inputs\n",
      "\n",
      " Iteration 1\n",
      "updating inputs\n",
      "\n",
      " Iteration 2\n",
      "updating inputs\n",
      "[[' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the school library and was hooked. It was hard, I had no clue what Harry Potter was. I', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the school library and was hooked. It was hard, I had no clue what Harry Potter was, and', ' So I was a junior in college when I got my first copy of Harry Potter.\\n\\nThere was no one else who knew about the books. I read every book in the school library and was hooked. It was hard, I had no clue what to do, and the']]\n"
     ]
    }
   ],
   "source": [
    "num_candidates = 3\n",
    "\n",
    "# \"sample\" means the input text that we use as the starting point for prediction\n",
    "tr_start = 0  # TR to start the sample at\n",
    "n_trs_to_sample = 3  # duration (in TRs) of the sample\n",
    "n_trs_to_test = 3  # how much further (in TRs) past the sample do you want to predict?\n",
    "max_depth = 3\n",
    "\n",
    "sample_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample].index  # token indices corresponding to sample\n",
    "test_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample+n_trs_to_test].index  # token indices corresponding to test\n",
    "new_token_idx = [x for x in test_idx if x not in sample_idx]  # the token indices that are in the test but not the sample\n",
    "\n",
    "sample_text = \" \".join(list(transcript_gpt2['word'][sample_idx.min():sample_idx.max()]))\n",
    "ground_truth = \" \".join(list(transcript_gpt2['word'][test_idx.min():test_idx.max()]))\n",
    "\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "try:\n",
    "  del sorted_generations\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for depth in range(max_depth):\n",
    "  next_inputs = []\n",
    "  print(next_inputs)\n",
    "  for i in range(num_candidates):\n",
    "    print('\\n', 'Iteration', i)\n",
    "    try:\n",
    "      inputs = tokenizer(sorted_generations[i], return_tensors=\"pt\").to(device)\n",
    "      print('updating inputs')\n",
    "      # breakpoint()\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "    # Generate continuations using nucleus sampling (https://huggingface.co/blog/how-to-generate)\n",
    "    beam_outputs = model.generate(**inputs,\n",
    "                                  num_return_sequences=num_candidates,\n",
    "                                  max_new_tokens=len(new_token_idx),  # cheating because we know the number of tokens we want to predict (would be replaced by word rate model)\n",
    "                                  do_sample=True,\n",
    "                                  top_p=0.92,\n",
    "                                  top_k=20,\n",
    "                                  temperature=1.0,\n",
    "                                  pad_token_id=50256)\n",
    "\n",
    "    # Decode the generated sequences\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in beam_outputs]\n",
    "\n",
    "    all_pred_bold = []\n",
    "    for text in generated_texts:\n",
    "        # 1. Tokenize the generated text:\n",
    "        tokenized_text = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # 2. Get embeddings from GPT-2:\n",
    "        with torch.no_grad():\n",
    "            output = model(tokenized_text, output_hidden_states=True)\n",
    "            embeddings = output.hidden_states[8][0].cpu().numpy()  # layer 8\n",
    "            # print(embeddings.shape)\n",
    "\n",
    "        # 3. Predict BOLD activity using fitted encoding model\n",
    "        delay = 0\n",
    "        predicted_bold = embeddings @ weights[768*delay:768*(delay+1)]  # weights have 4 delay options\n",
    "        all_pred_bold.append(predicted_bold)\n",
    "\n",
    "    # Rank predictions based on similarity to true BOLD data\n",
    "    from scipy.stats import pearsonr\n",
    "\n",
    "    # index pred_bold at new tokens only; index Y_parcels at new TRs only\n",
    "    similarities = [pearsonr(pred_bold[-1*len(new_token_idx):].mean(axis=0),\n",
    "                            Y_parcels[tr_start+n_trs_to_sample:tr_start+n_trs_to_sample+n_trs_to_test].mean(axis=0))[0]\n",
    "                    for pred_bold in all_pred_bold]\n",
    "\n",
    "    sorted_indices = np.argsort(similarities)[::-1] # [::-1] reverses a list so it's in descending order of similarity\n",
    "\n",
    "    # print('Input: ', tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))\n",
    "    # print('True continuation:', '\\n', ground_truth, '\\n')\n",
    "\n",
    "    # Print ranked continuations and their similarities\n",
    "    # for s, idx in enumerate(sorted_indices):\n",
    "    #     print(f\"Rank #{s+1}, candidate #{idx+1}, similarity={similarities[idx]:.4f}:\")\n",
    "    #     print(generated_texts[idx] + '\\n')\n",
    "\n",
    "    sorted_generations = [generated_texts[i] for i in sorted_indices]\n",
    "    # print(sorted_generations)\n",
    "\n",
    "  next_inputs.append(sorted_generations)\n",
    "  print(next_inputs)\n",
    "  inputs = next_inputs\n",
    "\n",
    "    # print(sorted_generations)\n",
    "    # print(len(sorted_generations))\n",
    "    # if len(sorted_generations) > num_candidates*5:\n",
    "    #     sorted_generations = sorted_generations[:num_candidates*5]  # prune the candidates post-sorting to prevent the number from growing too fast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tDuxQT1XU7U8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "tDuxQT1XU7U8",
    "outputId": "e2f0145f-92ed-4bc4-b9c1-df726303d2cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' So I was a junior in college when I got my'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QsUcedVyQyI_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsUcedVyQyI_",
    "outputId": "38e57653-52c3-4a2f-8c5e-ec364541ea8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEPTH 0, BRANCH 0\n",
      "INPUT: So I was a junior in college when I got my\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0:\n",
      "\n",
      "Candidate 1, Similarity: 0.4248\n",
      "Text:  So I was a junior in college when I got my first shot at playing a\n",
      "\n",
      "Candidate 2, Similarity: 0.3965\n",
      "Text:  So I was a junior in college when I got my first job, and we\n",
      "\n",
      "Candidate 3, Similarity: 0.3932\n",
      "Text:  So I was a junior in college when I got my first call at age 10\n",
      "\n",
      "Selected for next depth - Branch 0.0\n",
      "Similarity: 0.4248\n",
      "Text:  So I was a junior in college when I got my first shot at playing a\n",
      "\n",
      "Selected for next depth - Branch 0.1\n",
      "Similarity: 0.3965\n",
      "Text:  So I was a junior in college when I got my first job, and we\n",
      "\n",
      "Selected for next depth - Branch 0.2\n",
      "Similarity: 0.3932\n",
      "Text:  So I was a junior in college when I got my first call at age 10\n",
      "\n",
      "################################################################################\n",
      "COMPLETED DEPTH 0\n",
      "Number of branches for next depth: 3\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "DEPTH 1, BRANCH 0.0\n",
      "INPUT:  So I was a junior in college when I got my first shot at playing a\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.0:\n",
      "\n",
      "Candidate 1, Similarity: 0.2089\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a\n",
      "\n",
      "Candidate 2, Similarity: 0.2625\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I\n",
      "\n",
      "Candidate 3, Similarity: 0.2991\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life\n",
      "\n",
      "Selected for next depth - Branch 0.0.0\n",
      "Similarity: 0.2991\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life\n",
      "\n",
      "Selected for next depth - Branch 0.0.1\n",
      "Similarity: 0.2625\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I\n",
      "\n",
      "Selected for next depth - Branch 0.0.2\n",
      "Similarity: 0.2089\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a\n",
      "\n",
      "================================================================================\n",
      "DEPTH 1, BRANCH 0.1\n",
      "INPUT:  So I was a junior in college when I got my first job, and we\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.1:\n",
      "\n",
      "Candidate 1, Similarity: 0.2266\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together\n",
      "\n",
      "Candidate 2, Similarity: 0.2981\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse\n",
      "\n",
      "Candidate 3, Similarity: 0.1849\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where\n",
      "\n",
      "Selected for next depth - Branch 0.1.0\n",
      "Similarity: 0.2981\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse\n",
      "\n",
      "Selected for next depth - Branch 0.1.1\n",
      "Similarity: 0.2266\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together\n",
      "\n",
      "Selected for next depth - Branch 0.1.2\n",
      "Similarity: 0.1849\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where\n",
      "\n",
      "================================================================================\n",
      "DEPTH 1, BRANCH 0.2\n",
      "INPUT:  So I was a junior in college when I got my first call at age 10\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.2:\n",
      "\n",
      "Candidate 1, Similarity: 0.0706\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this\n",
      "\n",
      "Candidate 2, Similarity: 0.3402\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior\n",
      "\n",
      "Candidate 3, Similarity: 0.3571\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior\n",
      "\n",
      "Selected for next depth - Branch 0.2.0\n",
      "Similarity: 0.3571\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior\n",
      "\n",
      "Selected for next depth - Branch 0.2.1\n",
      "Similarity: 0.3402\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior\n",
      "\n",
      "Selected for next depth - Branch 0.2.2\n",
      "Similarity: 0.0706\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this\n",
      "\n",
      "################################################################################\n",
      "COMPLETED DEPTH 1\n",
      "Number of branches for next depth: 9\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.0.0\n",
      "INPUT:  So I was a junior in college when I got my first shot at playing a big role in my life\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.0.0:\n",
      "\n",
      "Candidate 1, Similarity: 0.1960\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life,\" said the 28-\n",
      "\n",
      "Candidate 2, Similarity: 0.0969\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life.\"\n",
      "\n",
      "He added\n",
      "\n",
      "Candidate 3, Similarity: 0.0961\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life. And the first shot\n",
      "\n",
      "Selected for next depth - Branch 0.0.0.0\n",
      "Similarity: 0.1960\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life,\" said the 28-\n",
      "\n",
      "Selected for next depth - Branch 0.0.0.1\n",
      "Similarity: 0.0969\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life.\"\n",
      "\n",
      "He added\n",
      "\n",
      "Selected for next depth - Branch 0.0.0.2\n",
      "Similarity: 0.0961\n",
      "Text:  So I was a junior in college when I got my first shot at playing a big role in my life. And the first shot\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.0.1\n",
      "INPUT:  So I was a junior in college when I got my first shot at playing a small forward. When I\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.0.1:\n",
      "\n",
      "Candidate 1, Similarity: 0.3717\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I went to junior college,\n",
      "\n",
      "Candidate 2, Similarity: 0.2034\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I saw my first shot at\n",
      "\n",
      "Candidate 3, Similarity: 0.2352\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I got older, I played\n",
      "\n",
      "Selected for next depth - Branch 0.0.1.0\n",
      "Similarity: 0.3717\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I went to junior college,\n",
      "\n",
      "Selected for next depth - Branch 0.0.1.1\n",
      "Similarity: 0.2352\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I got older, I played\n",
      "\n",
      "Selected for next depth - Branch 0.0.1.2\n",
      "Similarity: 0.2034\n",
      "Text:  So I was a junior in college when I got my first shot at playing a small forward. When I saw my first shot at\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.0.2\n",
      "INPUT:  So I was a junior in college when I got my first shot at playing a lot. I played a\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.0.2:\n",
      "\n",
      "Candidate 1, Similarity: 0.1936\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot of ball, but\n",
      "\n",
      "Candidate 2, Similarity: 0.3032\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot of college hockey.\n",
      "\n",
      "Candidate 3, Similarity: 0.2462\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot for the Chicago Bears\n",
      "\n",
      "Selected for next depth - Branch 0.0.2.0\n",
      "Similarity: 0.3032\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot of college hockey.\n",
      "\n",
      "Selected for next depth - Branch 0.0.2.1\n",
      "Similarity: 0.2462\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot for the Chicago Bears\n",
      "\n",
      "Selected for next depth - Branch 0.0.2.2\n",
      "Similarity: 0.1936\n",
      "Text:  So I was a junior in college when I got my first shot at playing a lot. I played a lot of ball, but\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.1.0\n",
      "INPUT:  So I was a junior in college when I got my first job, and we were working in a warehouse\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.1.0:\n",
      "\n",
      "Candidate 1, Similarity: 0.0381\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse on the side of the\n",
      "\n",
      "Candidate 2, Similarity: 0.1395\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse in an office building on\n",
      "\n",
      "Candidate 3, Similarity: 0.1314\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse and I got an opportunity\n",
      "\n",
      "Selected for next depth - Branch 0.1.0.0\n",
      "Similarity: 0.1395\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse in an office building on\n",
      "\n",
      "Selected for next depth - Branch 0.1.0.1\n",
      "Similarity: 0.1314\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse and I got an opportunity\n",
      "\n",
      "Selected for next depth - Branch 0.1.0.2\n",
      "Similarity: 0.0381\n",
      "Text:  So I was a junior in college when I got my first job, and we were working in a warehouse on the side of the\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.1.1\n",
      "INPUT:  So I was a junior in college when I got my first job, and we were all in it together\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.1.1:\n",
      "\n",
      "Candidate 1, Similarity: 0.1059\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together, and I got a\n",
      "\n",
      "Candidate 2, Similarity: 0.0728\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together. And then we started\n",
      "\n",
      "Candidate 3, Similarity: 0.1364\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together. We had a little\n",
      "\n",
      "Selected for next depth - Branch 0.1.1.0\n",
      "Similarity: 0.1364\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together. We had a little\n",
      "\n",
      "Selected for next depth - Branch 0.1.1.1\n",
      "Similarity: 0.1059\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together, and I got a\n",
      "\n",
      "Selected for next depth - Branch 0.1.1.2\n",
      "Similarity: 0.0728\n",
      "Text:  So I was a junior in college when I got my first job, and we were all in it together. And then we started\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.1.2\n",
      "INPUT:  So I was a junior in college when I got my first job, and we had an open house where\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.1.2:\n",
      "\n",
      "Candidate 1, Similarity: 0.0520\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I got to meet people\n",
      "\n",
      "Candidate 2, Similarity: 0.0413\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I went on an open\n",
      "\n",
      "Candidate 3, Similarity: 0.0370\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I had to go out\n",
      "\n",
      "Selected for next depth - Branch 0.1.2.0\n",
      "Similarity: 0.0520\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I got to meet people\n",
      "\n",
      "Selected for next depth - Branch 0.1.2.1\n",
      "Similarity: 0.0413\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I went on an open\n",
      "\n",
      "Selected for next depth - Branch 0.1.2.2\n",
      "Similarity: 0.0370\n",
      "Text:  So I was a junior in college when I got my first job, and we had an open house where I had to go out\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.2.0\n",
      "INPUT:  So I was a junior in college when I got my first call at age 10 to my high school senior\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.2.0:\n",
      "\n",
      "Candidate 1, Similarity: 0.1968\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year. I was sitting\n",
      "\n",
      "Candidate 2, Similarity: 0.3178\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year in high school.\n",
      "\n",
      "Candidate 3, Similarity: 0.1712\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year. The first thing\n",
      "\n",
      "Selected for next depth - Branch 0.2.0.0\n",
      "Similarity: 0.3178\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year in high school.\n",
      "\n",
      "Selected for next depth - Branch 0.2.0.1\n",
      "Similarity: 0.1968\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year. I was sitting\n",
      "\n",
      "Selected for next depth - Branch 0.2.0.2\n",
      "Similarity: 0.1712\n",
      "Text:  So I was a junior in college when I got my first call at age 10 to my high school senior year. The first thing\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.2.1\n",
      "INPUT:  So I was a junior in college when I got my first call at age 10: I was a senior\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.2.1:\n",
      "\n",
      "Candidate 1, Similarity: 0.3923\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior in high school when we\n",
      "\n",
      "Candidate 2, Similarity: 0.2157\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior, and I was a\n",
      "\n",
      "Candidate 3, Similarity: 0.3651\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior in college when I got\n",
      "\n",
      "Selected for next depth - Branch 0.2.1.0\n",
      "Similarity: 0.3923\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior in high school when we\n",
      "\n",
      "Selected for next depth - Branch 0.2.1.1\n",
      "Similarity: 0.3651\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior in college when I got\n",
      "\n",
      "Selected for next depth - Branch 0.2.1.2\n",
      "Similarity: 0.2157\n",
      "Text:  So I was a junior in college when I got my first call at age 10: I was a senior, and I was a\n",
      "\n",
      "================================================================================\n",
      "DEPTH 2, BRANCH 0.2.2\n",
      "INPUT:  So I was a junior in college when I got my first call at age 10, and there was this\n",
      "================================================================================\n",
      "\n",
      "CANDIDATES FOR BRANCH 0.2.2:\n",
      "\n",
      "Candidate 1, Similarity: 0.0361\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this one thing on my mind\n",
      "\n",
      "Candidate 2, Similarity: 0.0068\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this guy named John and he\n",
      "\n",
      "Candidate 3, Similarity: 0.1005\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this girl I liked who had\n",
      "\n",
      "Selected for next depth - Branch 0.2.2.0\n",
      "Similarity: 0.1005\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this girl I liked who had\n",
      "\n",
      "Selected for next depth - Branch 0.2.2.1\n",
      "Similarity: 0.0361\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this one thing on my mind\n",
      "\n",
      "Selected for next depth - Branch 0.2.2.2\n",
      "Similarity: 0.0068\n",
      "Text:  So I was a junior in college when I got my first call at age 10, and there was this guy named John and he\n",
      "\n",
      "################################################################################\n",
      "COMPLETED DEPTH 2\n",
      "Number of branches for next depth: 27\n",
      "################################################################################\n",
      "\n",
      "Generation Complete!\n"
     ]
    }
   ],
   "source": [
    "num_candidates = 3\n",
    "max_depth = 3\n",
    "\n",
    "tr_start = 0\n",
    "n_trs_to_sample = 3\n",
    "n_trs_to_test = 3\n",
    "\n",
    "sample_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample].index\n",
    "test_idx = transcript_gpt2[transcript_gpt2['TR'] >= tr_start][transcript_gpt2['TR'] <= tr_start+n_trs_to_sample+n_trs_to_test].index\n",
    "new_token_idx = [x for x in test_idx if x not in sample_idx]\n",
    "\n",
    "sample_text = \" \".join(list(transcript_gpt2['word'][sample_idx.min():sample_idx.max()]))\n",
    "ground_truth = \" \".join(list(transcript_gpt2['word'][test_idx.min():test_idx.max()]))\n",
    "\n",
    "# Initialize with first input\n",
    "current_inputs = [{'text': sample_text, 'depth': 0, 'branch_id': '0'}]\n",
    "\n",
    "# Loop through depths\n",
    "for depth in range(max_depth):\n",
    "    next_inputs = []  # Will store inputs for next depth level\n",
    "\n",
    "    # Process each input at current depth\n",
    "    for input_idx, current_input in enumerate(current_inputs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"DEPTH {depth}, BRANCH {current_input['branch_id']}\")\n",
    "        print(f\"INPUT: {current_input['text']}\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        # Tokenize current input\n",
    "        inputs = tokenizer(current_input['text'], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate continuations\n",
    "        beam_outputs = model.generate(**inputs,\n",
    "                                    num_return_sequences=num_candidates,\n",
    "                                    max_new_tokens=len(new_token_idx),\n",
    "                                    do_sample=True,\n",
    "                                    top_p=0.92,\n",
    "                                    top_k=20,\n",
    "                                    temperature=1.0,\n",
    "                                    pad_token_id=50256)\n",
    "\n",
    "        # Decode the generated sequences\n",
    "        generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in beam_outputs]\n",
    "\n",
    "        # Score generations\n",
    "        all_pred_bold = []\n",
    "        for text in generated_texts:\n",
    "            tokenized_text = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(tokenized_text, output_hidden_states=True)\n",
    "                embeddings = output.hidden_states[8][0].cpu().numpy()\n",
    "\n",
    "            delay = 0\n",
    "            predicted_bold = embeddings @ weights[768*delay:768*(delay+1)]\n",
    "            all_pred_bold.append(predicted_bold)\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = [pearsonr(pred_bold[-1*len(new_token_idx):].mean(axis=0),\n",
    "                               Y_parcels[tr_start+n_trs_to_sample:tr_start+n_trs_to_sample+n_trs_to_test].mean(axis=0))[0]\n",
    "                      for pred_bold in all_pred_bold]\n",
    "\n",
    "        # Print all candidates for this branch\n",
    "        print(f\"\\nCANDIDATES FOR BRANCH {current_input['branch_id']}:\")\n",
    "        for i, (text, similarity) in enumerate(zip(generated_texts, similarities)):\n",
    "            print(f\"\\nCandidate {i+1}, Similarity: {similarity:.4f}\")\n",
    "            print(f\"Text: {text}\")\n",
    "\n",
    "        # Sort generations by similarity\n",
    "        sorted_indices = np.argsort(similarities)[::-1]\n",
    "        sorted_generations = [generated_texts[i] for i in sorted_indices]\n",
    "        sorted_similarities = [similarities[i] for i in sorted_indices]\n",
    "\n",
    "        # Add top candidates as inputs for next depth\n",
    "        for i, (gen, sim) in enumerate(zip(sorted_generations[:num_candidates],\n",
    "                                         sorted_similarities[:num_candidates])):\n",
    "            new_branch_id = f\"{current_input['branch_id']}.{i}\"\n",
    "            next_inputs.append({\n",
    "                'text': gen,\n",
    "                'depth': depth + 1,\n",
    "                'branch_id': new_branch_id,\n",
    "                'similarity': sim\n",
    "            })\n",
    "\n",
    "            print(f\"\\nSelected for next depth - Branch {new_branch_id}\")\n",
    "            print(f\"Similarity: {sim:.4f}\")\n",
    "            print(f\"Text: {gen}\")\n",
    "\n",
    "    # Update current_inputs for next depth\n",
    "    current_inputs = next_inputs\n",
    "\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"COMPLETED DEPTH {depth}\")\n",
    "    print(f\"Number of branches for next depth: {len(current_inputs)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "\n",
    "print(\"\\nGeneration Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YXuuzTRVKCPm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXuuzTRVKCPm",
    "outputId": "e60e550a-0a2f-4d07-a7d0-47992155406d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bold[-1*len(new_token_idx):].mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MKg8oBtcko7p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "MKg8oBtcko7p",
    "outputId": "edc9c5b2-22cd-4ff7-a53c-2dc35f5b9d2e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"transcript_gpt2[transcript_gpt2['TR'] <= 7]\",\n  \"rows\": 21,\n  \"fields\": [\n    {\n      \"column\": \"word_index\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0,\n          17,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"So\",\n          \"I\",\n          \"got\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"onset\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.23999999463558197,\n          10.5,\n          9.210000038146973\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"offset\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 21,\n        \"samples\": [\n          0.6299999952316284,\n          10.619999885559082,\n          9.989999771118164\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          \"\\u0120So\",\n          \"\\u0120I\",\n          \"\\u0120got\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"token_id\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 18,\n        \"samples\": [\n          1406,\n          314,\n          1392\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rank\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          40.0,\n          27.0,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_prob\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.01734090782701969,\n          0.002966772299259901,\n          0.003786969231441617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_pred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"<|endoftext|>\",\n          \",\",\n          \"\\u0120at\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entropy\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          7.148902416229248,\n          7.2332258224487305,\n          4.133007049560547\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1,\n          5,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-66e02bb2-2932-4765-adcd-e9e6beac54ef\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_index</th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>true_prob</th>\n",
       "      <th>top_pred</th>\n",
       "      <th>entropy</th>\n",
       "      <th>embedding</th>\n",
       "      <th>TR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>ĠSo</td>\n",
       "      <td>1406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;|endoftext|&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1.0905547, -0.17344014, 0.91752285, 0.1744757...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.260000</td>\n",
       "      <td>ĠI</td>\n",
       "      <td>314</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>,</td>\n",
       "      <td>7.148902</td>\n",
       "      <td>[0.5017998, -2.0902424, 3.5390282, 0.30429798,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>was</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>Ġwas</td>\n",
       "      <td>373</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>'m</td>\n",
       "      <td>4.817344</td>\n",
       "      <td>[-1.1007988, -0.7025111, 0.96589136, 0.0476092...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>Ġa</td>\n",
       "      <td>257</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.022548</td>\n",
       "      <td>Ġjust</td>\n",
       "      <td>5.935095</td>\n",
       "      <td>[-1.329814, 0.8528788, 2.9786367, -5.1064243, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>junior</td>\n",
       "      <td>2.460000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>Ġjunior</td>\n",
       "      <td>13430</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>Ġlittle</td>\n",
       "      <td>5.770852</td>\n",
       "      <td>[4.4429436, 2.796859, -5.1273956, -1.6790009, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>in</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.165316</td>\n",
       "      <td>Ġat</td>\n",
       "      <td>4.166420</td>\n",
       "      <td>[3.6545837, -2.2084537, -0.3126945, -1.4611255...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>college</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>Ġcollege</td>\n",
       "      <td>4152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.154319</td>\n",
       "      <td>Ġhigh</td>\n",
       "      <td>1.320331</td>\n",
       "      <td>[1.9245641, -0.4866644, -3.7626815, 1.6225989,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>when</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>Ġwhen</td>\n",
       "      <td>618</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.071544</td>\n",
       "      <td>Ġand</td>\n",
       "      <td>2.541404</td>\n",
       "      <td>[1.2996331, -0.6207447, -0.16586041, 0.8665891...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>I</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>ĠI</td>\n",
       "      <td>314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403896</td>\n",
       "      <td>ĠI</td>\n",
       "      <td>3.931901</td>\n",
       "      <td>[0.8777293, -1.5289568, 2.099099, -1.530497, -...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>got</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>Ġgot</td>\n",
       "      <td>1392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.079014</td>\n",
       "      <td>Ġwas</td>\n",
       "      <td>4.176659</td>\n",
       "      <td>[-2.048574, -6.086743, 2.702452, -4.5428843, 0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>my</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>5.590000</td>\n",
       "      <td>Ġmy</td>\n",
       "      <td>616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.117469</td>\n",
       "      <td>Ġmy</td>\n",
       "      <td>4.711077</td>\n",
       "      <td>[-1.600373, -3.3452868, -0.22721505, -0.631124...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>first</td>\n",
       "      <td>5.610000</td>\n",
       "      <td>6.340000</td>\n",
       "      <td>Ġfirst</td>\n",
       "      <td>717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341477</td>\n",
       "      <td>Ġfirst</td>\n",
       "      <td>5.093065</td>\n",
       "      <td>[3.0442052, -2.5368137, -1.2964854, -3.7536855...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>paying</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.630000</td>\n",
       "      <td>Ġpaying</td>\n",
       "      <td>5989</td>\n",
       "      <td>6685.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>Ġjob</td>\n",
       "      <td>6.668958</td>\n",
       "      <td>[1.2421129, -5.8519526, -0.38289535, -5.101775...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>in</td>\n",
       "      <td>7.640000</td>\n",
       "      <td>7.660000</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>287</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>Ġjob</td>\n",
       "      <td>2.206553</td>\n",
       "      <td>[2.6938996, -4.0571914, -0.09501997, -5.767402...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>my</td>\n",
       "      <td>8.929999</td>\n",
       "      <td>9.199999</td>\n",
       "      <td>Ġmy</td>\n",
       "      <td>616</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.017087</td>\n",
       "      <td>Ġthe</td>\n",
       "      <td>5.958385</td>\n",
       "      <td>[-1.0538337, -0.15403235, -1.3715365, -1.67282...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>field</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>9.990000</td>\n",
       "      <td>Ġfield</td>\n",
       "      <td>2214</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>Ġsenior</td>\n",
       "      <td>4.809063</td>\n",
       "      <td>[-1.7850589, -7.925581, -1.4074312, 0.624632, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>on</td>\n",
       "      <td>10.290000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>Ġon</td>\n",
       "      <td>319</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.003787</td>\n",
       "      <td>.</td>\n",
       "      <td>4.133007</td>\n",
       "      <td>[2.645558, -3.3891754, -1.1554047, -5.8873553,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>the</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>Ġthe</td>\n",
       "      <td>262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181414</td>\n",
       "      <td>Ġthe</td>\n",
       "      <td>5.066846</td>\n",
       "      <td>[-2.8640788, -0.42069596, -1.5673913, -0.51705...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>radio</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>Ġradio</td>\n",
       "      <td>5243</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>Ġfirst</td>\n",
       "      <td>7.233226</td>\n",
       "      <td>[-0.5283354, -4.8964596, 1.6569365, -0.8455816...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>This</td>\n",
       "      <td>11.820000</td>\n",
       "      <td>11.970000</td>\n",
       "      <td>ĠThis</td>\n",
       "      <td>770</td>\n",
       "      <td>505.0</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>.</td>\n",
       "      <td>2.495917</td>\n",
       "      <td>[3.624095, -1.9530191, 5.252742, -1.7729495, 0...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>is</td>\n",
       "      <td>11.969999</td>\n",
       "      <td>12.089999</td>\n",
       "      <td>Ġis</td>\n",
       "      <td>318</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216188</td>\n",
       "      <td>Ġis</td>\n",
       "      <td>5.359241</td>\n",
       "      <td>[-0.5406225, 3.1513321, 3.2393796, 3.9819725, ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66e02bb2-2932-4765-adcd-e9e6beac54ef')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-66e02bb2-2932-4765-adcd-e9e6beac54ef button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-66e02bb2-2932-4765-adcd-e9e6beac54ef');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-4fa028bf-c7a2-49b7-ad68-37aca682b20c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4fa028bf-c7a2-49b7-ad68-37aca682b20c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-4fa028bf-c7a2-49b7-ad68-37aca682b20c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    word_index     word      onset     offset     token  token_id    rank  \\\n",
       "0            0       So   0.240000   0.630000       ĠSo      1406     NaN   \n",
       "1            1        I   0.680000   1.260000        ĠI       314     6.0   \n",
       "2            2      was   1.960000   2.300000      Ġwas       373     3.0   \n",
       "3            3        a   2.300000   2.450000        Ġa       257     5.0   \n",
       "4            4   junior   2.460000   3.140000   Ġjunior     13430    53.0   \n",
       "5            5       in   3.140000   3.410000       Ġin       287     1.0   \n",
       "6            6  college   3.410000   4.200000  Ġcollege      4152     1.0   \n",
       "7            7     when   4.790000   5.020000     Ġwhen       618     3.0   \n",
       "8            8        I   5.020000   5.090000        ĠI       314     0.0   \n",
       "9            9      got   5.090000   5.330000      Ġgot      1392     1.0   \n",
       "10          10       my   5.350000   5.590000       Ġmy       616     0.0   \n",
       "11          11    first   5.610000   6.340000    Ġfirst       717     0.0   \n",
       "12          12   paying   7.110000   7.630000   Ġpaying      5989  6685.0   \n",
       "13          13       in   7.640000   7.660000       Ġin       287   149.0   \n",
       "14          14       my   8.929999   9.199999       Ġmy       616     5.0   \n",
       "15          15    field   9.210000   9.990000    Ġfield      2214   113.0   \n",
       "16          16       on  10.290000  10.500000       Ġon       319    27.0   \n",
       "17          17      the  10.500000  10.620000      Ġthe       262     0.0   \n",
       "18          18    radio  10.620000  11.360000    Ġradio      5243    40.0   \n",
       "19          19     This  11.820000  11.970000     ĠThis       770   505.0   \n",
       "20          20       is  11.969999  12.089999       Ġis       318     0.0   \n",
       "\n",
       "    true_prob       top_pred   entropy  \\\n",
       "0         NaN  <|endoftext|>       NaN   \n",
       "1    0.017341              ,  7.148902   \n",
       "2    0.039634             'm  4.817344   \n",
       "3    0.022548          Ġjust  5.935095   \n",
       "4    0.001607        Ġlittle  5.770852   \n",
       "5    0.165316            Ġat  4.166420   \n",
       "6    0.154319          Ġhigh  1.320331   \n",
       "7    0.071544           Ġand  2.541404   \n",
       "8    0.403896             ĠI  3.931901   \n",
       "9    0.079014           Ġwas  4.176659   \n",
       "10   0.117469            Ġmy  4.711077   \n",
       "11   0.341477         Ġfirst  5.093065   \n",
       "12   0.000008           Ġjob  6.668958   \n",
       "13   0.000197           Ġjob  2.206553   \n",
       "14   0.017087           Ġthe  5.958385   \n",
       "15   0.000741        Ġsenior  4.809063   \n",
       "16   0.003787              .  4.133007   \n",
       "17   0.181414           Ġthe  5.066846   \n",
       "18   0.002967         Ġfirst  7.233226   \n",
       "19   0.000020              .  2.495917   \n",
       "20   0.216188            Ġis  5.359241   \n",
       "\n",
       "                                            embedding  TR  \n",
       "0   [1.0905547, -0.17344014, 0.91752285, 0.1744757...   0  \n",
       "1   [0.5017998, -2.0902424, 3.5390282, 0.30429798,...   0  \n",
       "2   [-1.1007988, -0.7025111, 0.96589136, 0.0476092...   1  \n",
       "3   [-1.329814, 0.8528788, 2.9786367, -5.1064243, ...   1  \n",
       "4   [4.4429436, 2.796859, -5.1273956, -1.6790009, ...   1  \n",
       "5   [3.6545837, -2.2084537, -0.3126945, -1.4611255...   2  \n",
       "6   [1.9245641, -0.4866644, -3.7626815, 1.6225989,...   2  \n",
       "7   [1.2996331, -0.6207447, -0.16586041, 0.8665891...   3  \n",
       "8   [0.8777293, -1.5289568, 2.099099, -1.530497, -...   3  \n",
       "9   [-2.048574, -6.086743, 2.702452, -4.5428843, 0...   3  \n",
       "10  [-1.600373, -3.3452868, -0.22721505, -0.631124...   3  \n",
       "11  [3.0442052, -2.5368137, -1.2964854, -3.7536855...   3  \n",
       "12  [1.2421129, -5.8519526, -0.38289535, -5.101775...   4  \n",
       "13  [2.6938996, -4.0571914, -0.09501997, -5.767402...   5  \n",
       "14  [-1.0538337, -0.15403235, -1.3715365, -1.67282...   5  \n",
       "15  [-1.7850589, -7.925581, -1.4074312, 0.624632, ...   6  \n",
       "16  [2.645558, -3.3891754, -1.1554047, -5.8873553,...   6  \n",
       "17  [-2.8640788, -0.42069596, -1.5673913, -0.51705...   7  \n",
       "18  [-0.5283354, -4.8964596, 1.6569365, -0.8455816...   7  \n",
       "19  [3.624095, -1.9530191, 5.252742, -1.7729495, 0...   7  \n",
       "20  [-0.5406225, 3.1513321, 3.2393796, 3.9819725, ...   7  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what text data am i using?\n",
    "sample_text = \" \".join(list(transcript_gpt2['word'][input_start : input_start+n_sample_tokens]))\n",
    "\n",
    "# what are the corresponding TRs for the input data?\n",
    "transcript_gpt2[transcript_gpt2['TR'] <= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3xEOaJBJcS2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G3xEOaJBJcS2",
    "outputId": "d6f494ae-83e9-492e-b261-038f343405f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_gpt2.TR.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iNlSIYXWOMYN",
   "metadata": {
    "id": "iNlSIYXWOMYN"
   },
   "source": [
    "For illustrative purposes, we'll plot the actual and predicted time series for an example parcel. Finally, we'll use the `masker`'s `.inverse_transform()` method to convert the parcel-level correlation scores back to into a brain image for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20a73d-c247-464a-a541-eb99a5eb292a",
   "metadata": {
    "id": "2b20a73d-c247-464a-a541-eb99a5eb292a"
   },
   "outputs": [],
   "source": [
    "# Plot predicted and actual response for example parcel:\n",
    "example_parcel = 195\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(zscore(Y_parcels[:, example_parcel]), label='actual')\n",
    "ax.plot(zscore(Y_predicted[:, example_parcel]), label='predicted')\n",
    "ax.annotate(f'$\\it{{r}}$ = {score_parcels[example_parcel]:.3f}',\n",
    "            xy=(10, 10), xycoords='axes points', fontsize=12)\n",
    "ax.set(xlabel='TRs', ylabel='activity', xlim=(0, Y_predicted.shape[0]))\n",
    "ax.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186fe04-e46c-43ed-86b9-f854f744c2cd",
   "metadata": {
    "id": "9186fe04-e46c-43ed-86b9-f854f744c2cd"
   },
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain\n",
    "score_img = masker.inverse_transform(score_parcels)\n",
    "\n",
    "# Plot encoding performance correlations on brain\n",
    "vmax = .4\n",
    "threshold = .1\n",
    "plot_stat_map(score_img, cmap='RdYlBu_r', vmax=vmax, threshold=threshold,\n",
    "              cut_coords=(-55, -24, 9))\n",
    "\n",
    "# Plot correlations to visualize posterior medial cortex\n",
    "plot_stat_map(score_img, cmap='RdYlBu_r', vmax=vmax, threshold=threshold,\n",
    "              cut_coords=(5, -60, 33));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639babe2-03ed-4bad-bed3-d03e9ddbcba3",
   "metadata": {
    "id": "639babe2-03ed-4bad-bed3-d03e9ddbcba3"
   },
   "outputs": [],
   "source": [
    "from surfplot.utils import threshold\n",
    "\n",
    "# Convert volumetric MNI data to fsLR surface\n",
    "gii_lh, gii_rh = mni152_to_fslr(score_img, method='nearest')\n",
    "\n",
    "# Threshold at an arbitrary correlation value\n",
    "gii_lh = threshold(gii_lh.agg_data(), .1)\n",
    "gii_rh = threshold(gii_rh.agg_data(), .1)\n",
    "\n",
    "# Plot example ROI on surface\n",
    "p = Plot(surf_lh=lh, surf_rh=rh, brightness=.7)\n",
    "p.add_layer({'left': gii_lh, 'right': gii_rh}, cmap='RdYlBu_r', color_range=(-.4, .4))\n",
    "cbar_kws = dict(location='right', draw_border=False, aspect=10,\n",
    "                shrink=.2, decimals=1, pad=0, n_ticks=2)\n",
    "fig = p.build(cbar_kws=cbar_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CupNb-J2Okbh",
   "metadata": {
    "id": "CupNb-J2Okbh"
   },
   "source": [
    "### Banded ridge regression\n",
    "In the previous section, we fit separate models for the word2vec and GPT-2 embeddings. However, these embeddings likely encode a lot of similar information, making it difficult to compare the feature spaces. One way to compare feature spaces is to fit both feature spaces jointly, allowing them both to vie for variance in the brain activity—then evaluate their predictions separately. Our first thought might be to horizontally stack the word2vec and GPT-2 features, then refit the model in the same as the previous section. The problem with this approach is that our encoding model will find a single hyperparameter for both feature spaces. If the feature spaces are qualitatively different from each other, the hyperparameter will likely be unfairly biased toward one feature space. For example, if one feature space is very high-dimensional (i.e. wide) and the other is not, the fitting procedure will likely find a strong penalty term to regularize the wide predictors—but this hyperparameter may overly \"squeeze\" the lower-dimensional predictors. To solve this problem, we'll use `himalaya`'s implementation of *banded ridge regression* to estimate a separate regularization parameter for each feature space (i.e. band) in the joint predictor matrix ([Nunez-Elizalde et al., 2019](https://doi.org/10.1016/j.neuroimage.2019.04.012); [Dupré la Tour et al., 2022](https://doi.org/10.1016/j.neuroimage.2022.119728)). The first step is to horizontally-stack our predictors for both feature space; we'll use `slice`s to keep track of which columns belong to which feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tzXDKPYHO1CL",
   "metadata": {
    "id": "tzXDKPYHO1CL"
   },
   "outputs": [],
   "source": [
    "# Horizontal-stack both embeddings to create joint model\n",
    "X_joint = np.hstack([X_w2v, X_gpt2])\n",
    "print(f\"Joint predictor matrix shape: {X_joint.shape}\")\n",
    "\n",
    "width_w2v = X_w2v.shape[1]\n",
    "width_gpt2 = X_gpt2.shape[1]\n",
    "\n",
    "slice_w2v = slice(0, width_w2v)\n",
    "slice_gpt2 = slice(width_w2v, width_w2v + width_gpt2)\n",
    "print(f\"word2vec slice: {slice_w2v}\")\n",
    "print(f\"GPT-2 slice: {slice_gpt2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3mhYlxpO4Nn",
   "metadata": {
    "id": "e3mhYlxpO4Nn"
   },
   "source": [
    "We'll set up the usual cross-validation scheme here. We need to take special care to apply the usual transforms—especially the kernelization step—to each set of predictors separately. We'll build a pipeline containing the necessary transforms, then use `himalaya`'s `ColumnKernelizer` apply this pipeline separately to both sets of predictors according to the `slice`s we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u_35IVDoO5vC",
   "metadata": {
    "id": "u_35IVDoO5vC"
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import Kernelizer, ColumnKernelizer\n",
    "\n",
    "# Split-half outer and inner cross-validation\n",
    "outer_cv = KFold(n_splits=2)\n",
    "inner_cv = KFold(n_splits=5)\n",
    "\n",
    "# Make pipeline with kernelizer for each feature space\n",
    "column_pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    Delayer(delays=[2, 3, 4, 5]),\n",
    "    Kernelizer(kernel=\"linear\"),\n",
    ")\n",
    "\n",
    "# Compile joint column kernelizer\n",
    "column_kernelizer = ColumnKernelizer(\n",
    "    [('word2vec', column_pipeline, slice_w2v),\n",
    "     ('gpt2', column_pipeline, slice_gpt2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CjajU0M0PCMI",
   "metadata": {
    "id": "CjajU0M0PCMI"
   },
   "source": [
    "We'll use `MultipleKernelRidgeCV` to implement banded ridge regression with hyperparameter search. Since banded ridge regression applies separate penalty terms to each feature space, we need to search over combinations of hyperparameters. This can become very computationally expensive, so we'll use random search to find a good combination of regularization parameters. We initialize the model and link it up to our `ColumnKernelizer` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-gCKO9OEPGSy",
   "metadata": {
    "id": "-gCKO9OEPGSy"
   },
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import MultipleKernelRidgeCV\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV\n",
    "solver = 'random_search'\n",
    "n_iter = 20\n",
    "alphas = np.logspace(1, 10, 10)\n",
    "solver_params = dict(n_iter=n_iter, alphas=alphas)\n",
    "\n",
    "# Banded ridge regression with column kernelizer\n",
    "banded_ridge = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n",
    "                                     solver_params=solver_params, cv=inner_cv)\n",
    "\n",
    "# Chain transfroms and estimator into pipeline\n",
    "pipeline = make_pipeline(column_kernelizer, banded_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4Dq9vmkPHUy",
   "metadata": {
    "id": "d4Dq9vmkPHUy"
   },
   "source": [
    "First, similarly to the previous exercise, we'll fit the joint model, generate a single set of predictions based on the combined feature spaces, and evaluate the quality of these predictions by computing the correlation between actual and predicted time series. This gives us an overall idea of how well the joint model predicts brain activity. In this approach, each set of predictors receives it's own regularization parameter, which should provide a better overall fit than if we had naively combined both sets of predictors under a single regularization parameter. Below, we compute the socres and visualize the results—but this isn't the end of the story! In the next section, we use banded ridge regression to compare feature spaces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bCTHsBg5PQ97",
   "metadata": {
    "id": "bCTHsBg5PQ97"
   },
   "outputs": [],
   "source": [
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "\n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X_joint[train],\n",
    "                 Y_parcels[train])\n",
    "\n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X_joint[test])\n",
    "    Y_predicted.append(predicted)\n",
    "\n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.vstack(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BZ2p_qXvPRIo",
   "metadata": {
    "id": "BZ2p_qXvPRIo"
   },
   "outputs": [],
   "source": [
    "from himalaya.scoring import correlation_score\n",
    "\n",
    "# Evaluate predictions: correlation between predicted and actual time series\n",
    "score_parcels = correlation_score(Y_predicted, Y_parcels)\n",
    "\n",
    "print(f\"Mean encoding performance: r = {np.mean(score_parcels):.3f}\")\n",
    "print(f\"Maximum encoding performance: r = {np.amax(score_parcels):.3f}\")\n",
    "\n",
    "# Plot a histogram of prediction performance values\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(score_parcels, stat='proportion', ax=ax)\n",
    "ax.set(xlabel='encoding performance (r)', ylabel='proportion of parcels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OvTOG9SNPRRO",
   "metadata": {
    "id": "OvTOG9SNPRRO"
   },
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain\n",
    "score_img = masker.inverse_transform(score_parcels)\n",
    "\n",
    "# Convert volumetric MNI data to fsLR surface\n",
    "gii_lh, gii_rh = mni152_to_fslr(score_img, method='nearest')\n",
    "\n",
    "# Threshold at an arbitrary correlation value\n",
    "gii_lh = threshold(gii_lh.agg_data(), .1)\n",
    "gii_rh = threshold(gii_rh.agg_data(), .1)\n",
    "\n",
    "# Plot example ROI on surface\n",
    "p = Plot(surf_lh=lh, surf_rh=rh, brightness=.7)\n",
    "p.add_layer({'left': gii_lh, 'right': gii_rh}, cmap='RdYlBu_r', color_range=(-.4, .4))\n",
    "cbar_kws = dict(location='right', draw_border=False, aspect=10,\n",
    "                shrink=.2, decimals=1, pad=0, n_ticks=2)\n",
    "fig = p.build(cbar_kws=cbar_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wuwCowhHPZEn",
   "metadata": {
    "id": "wuwCowhHPZEn"
   },
   "source": [
    "### Comparing feature spaces\n",
    "Previously we estimated the joint model containing both word2vec and GPT-2 embeddings and evaluated the performance of the combined feature spaces. However, the more important question is typically: How do these feature spaces compare to one another? Here, we fit the joint model in the same way as the previous section—but critically we generate predictions separately for each feature space (using `split=True` in the `.predict()` method of the pipeline culminating in `MultipleKernelRidgeCV`). Remember that we generate model-based predictions by multiplying the weight vector estimated from the training set with the predictors from the test set. In this approach, we effectively zero-out weights for the feature-space(s)-of-no-interest to quantify how much one feature space contributes to performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CfRgcQ-gP1zu",
   "metadata": {
    "id": "CfRgcQ-gP1zu"
   },
   "outputs": [],
   "source": [
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "\n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X_joint[train],\n",
    "                 Y_parcels[train])\n",
    "\n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X_joint[test], split=True)\n",
    "    Y_predicted.append(predicted)\n",
    "\n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.concatenate(Y_predicted, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ATnZ-5MjP763",
   "metadata": {
    "id": "ATnZ-5MjP763"
   },
   "source": [
    "We can use `himalaya`'s `correlation_score_split` to easily compute correlation scores for the predictions based on each feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_C6mOlgnP4K0",
   "metadata": {
    "id": "_C6mOlgnP4K0"
   },
   "outputs": [],
   "source": [
    "from himalaya.scoring import correlation_score_split\n",
    "\n",
    "# Compute correlation between predicted and actual for split predictions\n",
    "score_w2v, score_gpt2 = correlation_score_split(Y_parcels, Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DQSF5Gl6QA30",
   "metadata": {
    "id": "DQSF5Gl6QA30"
   },
   "source": [
    "Let's summarize and compare the performance of both feature spaces using both a histogram and brain maps. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OvvrZd5XQGX4",
   "metadata": {
    "id": "OvvrZd5XQGX4"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean word2vec encoding performance: r = {np.mean(score_w2v):.3f}\")\n",
    "print(f\"Maximum word2vec encoding performance: r = {np.amax(score_w2v):.3f}\")\n",
    "\n",
    "print(f\"Mean GPT-2 encoding performance: r = {np.mean(score_gpt2):.3f}\")\n",
    "print(f\"Maximum GPT-2 encoding performance: r = {np.amax(score_gpt2):.3f}\")\n",
    "\n",
    "# Plot a histogram of prediction performance values\n",
    "score_df = pd.DataFrame({'feature space': np.repeat(['word2vec', 'GPT-2'], n_parcels),\n",
    "                         'encoding performance (r)': np.hstack([score_w2v, score_gpt2])})\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(score_df, x='encoding performance (r)', hue='feature space',\n",
    "             stat='proportion', ax=ax)\n",
    "ax.set(xlabel='encoding performance (r)', ylabel='proportion of parcels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "woVxtHVgQHPX",
   "metadata": {
    "id": "woVxtHVgQHPX"
   },
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain\n",
    "score_img = masker.inverse_transform(score_w2v)\n",
    "\n",
    "# Convert volumetric MNI data to fsLR surface\n",
    "gii_lh, gii_rh = mni152_to_fslr(score_img, method='nearest')\n",
    "\n",
    "# Threshold at an arbitrary correlation value\n",
    "gii_lh = threshold(gii_lh.agg_data(), .1)\n",
    "gii_rh = threshold(gii_rh.agg_data(), .1)\n",
    "\n",
    "# Plot example ROI on surface\n",
    "p = Plot(surf_lh=lh, surf_rh=rh, brightness=.7)\n",
    "p.add_layer({'left': gii_lh, 'right': gii_rh}, cmap='RdYlBu_r', color_range=(-.4, .4))\n",
    "cbar_kws = dict(location='right', draw_border=False, aspect=10,\n",
    "                shrink=.2, decimals=1, pad=0, n_ticks=2)\n",
    "fig = p.build(cbar_kws=cbar_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pGZrv9KuQHbU",
   "metadata": {
    "id": "pGZrv9KuQHbU"
   },
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain\n",
    "score_img = masker.inverse_transform(score_gpt2)\n",
    "\n",
    "# Convert volumetric MNI data to fsLR surface\n",
    "gii_lh, gii_rh = mni152_to_fslr(score_img, method='nearest')\n",
    "\n",
    "# Threshold at an arbitrary correlation value\n",
    "gii_lh = threshold(gii_lh.agg_data(), .1)\n",
    "gii_rh = threshold(gii_rh.agg_data(), .1)\n",
    "\n",
    "# Plot example ROI on surface\n",
    "p = Plot(surf_lh=lh, surf_rh=rh, brightness=.7)\n",
    "p.add_layer({'left': gii_lh, 'right': gii_rh}, cmap='RdYlBu_r', color_range=(-.4, .4))\n",
    "cbar_kws = dict(location='right', draw_border=False, aspect=10,\n",
    "                shrink=.2, decimals=1, pad=0, n_ticks=2)\n",
    "fig = p.build(cbar_kws=cbar_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V182Bj0ATCxe",
   "metadata": {
    "id": "V182Bj0ATCxe"
   },
   "source": [
    "## Natural language comprehension ECoG dataset\n",
    "In our second example, we'll use electrocorticography (ECoG) data acquired in an epilepsy patient listening to a 30-minute podcast episode called \"[Monkey in the Middle](https://www.thisamericanlife.org/631/so-a-monkey-and-a-horse-walk-into-a-bar/act-one-0)\" from the *This American Life* podcast ([Goldstein et al., 2022](https://doi.org/10.1038/s41593-022-01026-4)). This dataset has been preprocessed use [MNE](https://mne.tools/stable/index.html): common-average referencing, notch filtering for line noise, bandpass filtered for high gamma (70–200 Hz). We begin by loading in the preprocessed data using MNE. For the sake of expedience, we'll estimate encoding models based on only the word2vec embeddings—but this example should be easily extensible to the more complex procedure described in the previous section. First, we'll load in the ECoG data and visualize the electrode locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufMxHRt1TD0K",
   "metadata": {
    "id": "ufMxHRt1TD0K"
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Load data into MNE \"Raw\" object\n",
    "raw = mne.io.read_raw('sub-717_task-monkey_desc-clean_ieeg.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A_R_3Mw1TSS9",
   "metadata": {
    "id": "A_R_3Mw1TSS9"
   },
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_markers\n",
    "\n",
    "# Plot the electrode coordinates\n",
    "node_coords = np.vstack([ch['loc'][:3] for ch in raw.info['chs']])\n",
    "node_values = np.ones(len(node_coords))\n",
    "plot_markers(node_values, node_coords,\n",
    "             node_vmin=0, node_vmax=1.5,\n",
    "             node_size=50, node_cmap='Blues',\n",
    "             display_mode='lyr', colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G3MpnNTHTV_c",
   "metadata": {
    "id": "G3MpnNTHTV_c"
   },
   "source": [
    "Now, we'll load in the transcript and assign word2vec embeddings to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jBU0RACaTcrg",
   "metadata": {
    "id": "jBU0RACaTcrg"
   },
   "outputs": [],
   "source": [
    "# Download 300-dimensional word2vec embeddings\n",
    "model_name = 'word2vec-google-news-300'\n",
    "model = gensim.downloader.load(model_name)\n",
    "\n",
    "# Load in transcript CSV file\n",
    "transcript_f = 'monkey_transcript.csv'\n",
    "transcript_w2v = pd.read_csv(transcript_f)\n",
    "\n",
    "# Convert words to lowercase\n",
    "transcript_w2v['word'] = transcript_w2v.word.str.lower()\n",
    "\n",
    "# Function to extract embeddings if available\n",
    "def get_vector(word):\n",
    "    if word in model.key_to_index:\n",
    "        return model.get_vector(word, norm=True).astype(np.float32)\n",
    "    return np.nan\n",
    "\n",
    "# Extract embedding for each word\n",
    "transcript_w2v['embedding'] = transcript_w2v.word.apply(get_vector)\n",
    "transcript_w2v = transcript_w2v.astype({'onset': 'float32', 'offset': 'float32'}, copy=False)\n",
    "\n",
    "# Print out words not found in vocabulary\n",
    "print(f'{(transcript_w2v.embedding.isna()).sum()} words not found:')\n",
    "print(transcript_w2v.word[transcript_w2v.embedding.isna()].value_counts())\n",
    "\n",
    "# Save transcript with embeddings using pickle\n",
    "with open('monkey_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(transcript_w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oe3WKkxqTf_w",
   "metadata": {
    "id": "oe3WKkxqTf_w"
   },
   "outputs": [],
   "source": [
    "# Clear gensim word2vec model out of memory to save RAM\n",
    "del model\n",
    "\n",
    "# Reload transcript with embeddings if already generated\n",
    "transcript_f = 'monkey_w2v.pkl'\n",
    "if exists(transcript_f):\n",
    "    with open(transcript_f, 'rb') as f:\n",
    "        transcript_w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-N-AE0AZThNn",
   "metadata": {
    "id": "-N-AE0AZThNn"
   },
   "source": [
    "MNE allows us to assign word stimuli, as well as their onsets and durations, directly to the `Raw` object. (Note that MNE treats annotations containing the word \"bad\" in a special way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F_VTKMtaYKjM",
   "metadata": {
    "id": "F_VTKMtaYKjM"
   },
   "outputs": [],
   "source": [
    "# Add word onsets as annotations to the raw object\n",
    "annotations = mne.Annotations(transcript_w2v.onset.to_numpy() / 512,\n",
    "                              (transcript_w2v.offset - transcript_w2v.onset).to_numpy() / 512,\n",
    "                              description=transcript_w2v.word.to_numpy())\n",
    "raw = raw.set_annotations(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a_3117wwYShq",
   "metadata": {
    "id": "a_3117wwYShq"
   },
   "outputs": [],
   "source": [
    "# Extract word onsets from the annotations\n",
    "events, event_id = mne.events_from_annotations(raw, regexp=None, verbose=False)\n",
    "word_ids = {word: id for id, word in event_id.items()}\n",
    "words = [word_ids[word_id] for word_id in events[:, 2]]\n",
    "print(f'First few words: \"{\" \".join(words[:9])}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V8nCPUSjZrDk",
   "metadata": {
    "id": "V8nCPUSjZrDk"
   },
   "source": [
    "We'll take advantage of MNE's tools for creating epochs around stimulus events to first visualize electrodes that respond to word onsets. We effectively extract fixed-width windows around each word onset, then average each electrode's signal across all words, yielding event-related potentials (ERPs) for word onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9A1pylQuYLYl",
   "metadata": {
    "id": "9A1pylQuYLYl"
   },
   "outputs": [],
   "source": [
    "# Convert events to epochs\n",
    "epochs = mne.Epochs(raw,\n",
    "                    events, event_id=event_id,\n",
    "                    event_repeated='drop',\n",
    "                    tmin=-0.5, tmax=1.0,\n",
    "                    picks='ecog',\n",
    "                    detrend=None,\n",
    "                    baseline=(None, 0),\n",
    "                    reject_by_annotation=False)\n",
    "epochs = epochs.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0jdHGyW4Z1gG",
   "metadata": {
    "id": "0jdHGyW4Z1gG"
   },
   "source": [
    "We'll focus in on handful of \"good\" electrodes with obvious ERPs for the purpose of visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ysRDo1R3Z3yu",
   "metadata": {
    "id": "ysRDo1R3Z3yu"
   },
   "outputs": [],
   "source": [
    "# Plot average response for one good electrode\n",
    "good_erp = [9, 36, 37, 38, 45, 46]\n",
    "example_electrode = 9\n",
    "\n",
    "epochs.plot_image(picks=[example_electrode]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2TG8odWHZ-An",
   "metadata": {
    "id": "2TG8odWHZ-An"
   },
   "outputs": [],
   "source": [
    "# Average epochs for evoked response across good electrodes\n",
    "evoked = epochs.average(picks=good_erp)\n",
    "evoked.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6vmWiAXLaASb",
   "metadata": {
    "id": "6vmWiAXLaASb"
   },
   "source": [
    "Now that we've convinced ourselves there's some word-related signal in the ECoG data, let's get back to the word embeddings. We'll compile our embeddings into a predictor matrix `X` and ensure that these embeddings match the ECoG epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k27062sdaHWl",
   "metadata": {
    "id": "k27062sdaHWl"
   },
   "outputs": [],
   "source": [
    "# Compile embeddings from transcript\n",
    "words_keep = np.where(transcript_w2v['embedding'].notna())[0]\n",
    "X_w2v = np.vstack(transcript_w2v['embedding'][words_keep])\n",
    "\n",
    "# Only include events where we have word embeddings\n",
    "events = events[words_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TOeqykfjaGvg",
   "metadata": {
    "id": "TOeqykfjaGvg"
   },
   "source": [
    "We'll recreate our epochs around each word with a wider window and minimal processing. We'll downsample the temporal resolution to 100 Hz; that is, 100 samples per second. For a window ranging from -2 seconds to +2 seconds relative to word onset, we should have 400 lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0sQdD9JxaNFh",
   "metadata": {
    "id": "0sQdD9JxaNFh"
   },
   "outputs": [],
   "source": [
    "# Recreate epochs without any processing (e.g., baseline correction)\n",
    "epochs = mne.Epochs(raw,\n",
    "                    events, event_repeated='drop',\n",
    "                    tmin=-2.0, tmax=2.0,\n",
    "                    picks='ecog',\n",
    "                    reject_by_annotation=None,\n",
    "                    detrend=None,\n",
    "                    baseline=None)\n",
    "epochs = epochs.load_data()\n",
    "\n",
    "# Resample here so we don't have so many lags\n",
    "epochs = epochs.resample(100)\n",
    "\n",
    "# Extract data from epochs object\n",
    "Y_lags = epochs._data\n",
    "print(f\"ECoG data matrix shape: {Y_lags.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ziOQ1SY1aNqM",
   "metadata": {
    "id": "ziOQ1SY1aNqM"
   },
   "source": [
    "We plan to fit encoding models at each electrode and for each lag, so we'll reshape our target matrix `Y` to horizontally stack both electrodes and lags along the second dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4EhSwd7PaR0U",
   "metadata": {
    "id": "4EhSwd7PaR0U"
   },
   "outputs": [],
   "source": [
    "# Get target matrix shape\n",
    "n_words, n_electrodes, n_lags = Y_lags.shape\n",
    "\n",
    "Y_lags = Y_lags.reshape(n_words, n_electrodes * n_lags)\n",
    "print(f\"ECoG data matrix shape: {Y_lags.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d69ib_aU8Z",
   "metadata": {
    "id": "a3d69ib_aU8Z"
   },
   "source": [
    "Finally, we'll use ridge regression to estimate the encoding model similarly to before. Here, don't need to duplicate predictor matrix at multiple lags due to the high temporal resolution of ECoG data acquisition. In this case, we have many more samples than the width of the word2vec model—so we use `RidgeCV` rather than `KernelRidgeCV`. To close, we fit the model, generate predictions, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IC9jA9UMaXci",
   "metadata": {
    "id": "IC9jA9UMaXci"
   },
   "outputs": [],
   "source": [
    "from himalaya.ridge import RidgeCV\n",
    "\n",
    "# Split-half outer and inner cross-validation\n",
    "outer_cv = KFold(n_splits=2, shuffle=False)\n",
    "inner_cv = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV\n",
    "alphas = np.logspace(1, 10, 10)\n",
    "\n",
    "# Chain transfroms and estimator into pipeline\n",
    "pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    RidgeCV(alphas=alphas, cv=inner_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IUjoo1gZaZ6F",
   "metadata": {
    "id": "IUjoo1gZaZ6F"
   },
   "outputs": [],
   "source": [
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_lags):\n",
    "\n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X_w2v[train],\n",
    "                 Y_lags[train])\n",
    "\n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X_w2v[test])\n",
    "    Y_predicted.append(predicted)\n",
    "\n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.vstack(Y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hcSFON-faZ9-",
   "metadata": {
    "id": "hcSFON-faZ9-"
   },
   "outputs": [],
   "source": [
    "# Evaluate predictions: correlation between predicted and actual time series\n",
    "scores_lags = correlation_score(Y_lags, Y_predicted)\n",
    "\n",
    "scores = scores_lags.reshape(n_electrodes, n_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vPnQNpNaadMO",
   "metadata": {
    "id": "vPnQNpNaadMO"
   },
   "outputs": [],
   "source": [
    "# Plot the correlation for one electrode on all lags\n",
    "example_electrode = 9\n",
    "\n",
    "lags = epochs.times\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(0, color='.8')\n",
    "ax.axvline(0, color='.8')\n",
    "ax.plot(lags, scores[example_electrode])\n",
    "ax.set(xlabel='lag (s)', ylabel='encoding performance (r)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yaulItoAadQG",
   "metadata": {
    "id": "yaulItoAadQG"
   },
   "outputs": [],
   "source": [
    "# Plot maximum correlation across lags for all electrodes\n",
    "node_values = scores.max(-1)\n",
    "vmax = np.quantile(scores.max(-1), .99)\n",
    "\n",
    "plot_markers(node_values, node_coords,\n",
    "             node_size=50, node_cmap='RdYlBu_r', display_mode='lyr',\n",
    "             node_vmin=-vmax, node_vmax=vmax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7066bb-a72b-40b1-814a-a2d5ee962daf",
   "metadata": {
    "id": "df7066bb-a72b-40b1-814a-a2d5ee962daf"
   },
   "source": [
    "## References\n",
    "* Dupré La Tour, T., Eickenberg, M., Nunez-Elizalde, A.O., & Gallant, J. L. (2022).\n",
    "Feature-space selection with banded ridge regression. *NeuroImage*, *264*, 119728. https://doi.org/10.1016/j.neuroimage.2022.119728\n",
    "\n",
    "* Dupré La Tour, T., Visconti di Oleggio Castello, M., & Gallant, J. L. (2023). Voxelwise modeling tutorials: an encoding model approach to functional MRI analysis.\n",
    "\n",
    "* Goldstein, A., Nastase, S. A.\\*, Zada, Z.\\*, Buchnik, E.\\*, Schain, M.\\*, Price, A.\\*, Aubrey, B.\\*, Feder, A.\\*, Emanual D.\\*, Cohen, A.\\*, Jensen, A.\\*, Gazula, H., Choe, G., Rao, A., Kim, C., Casto, C., Lora, F., Flinker, A., Devore, S., Doyle, W., Dugan, P., Friedman, D., Hassidim, A., Brenner, M., Matias, Y., Norman, K. A., Devinsky, O., & Hasson, U. (2022). Shared computational principles for language processing in humans and deep language models. *Nature Neuroscience*, *25*, 369–380. https://doi.org/10.1038/s41593-022-01026-4\n",
    "\n",
    "* Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. *Nature*, *532*(7600), 453–458. https://doi.org/10.1038/nature17637\n",
    "\n",
    "* Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, & K. Q. Weinberger (Eds.), *Advances in Neural Information Processing Systems 26* (pp. 3111–3119). Curran Associates. https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html\n",
    "\n",
    "* Naselaris, T., Kay, K. N., Nishimoto, S., & Gallant, J. L. (2011). Encoding and decoding in fMRI. *NeuroImage*, *56*(2), 400–410. https://doi.org/10.1016/j.neuroimage.2010.07.073\n",
    "\n",
    "* Nastase, S. A., Liu, Y.-F., Hillman, H., Zadbood, A., Hasenfratz, L., Keshavarzian, N., Chen, J., Honey, C. J., Yeshurun, Y., Regev, M., Nguyen, M., Chang, C. H. C., Baldassano, C., Lositsky, O., Simony, E., Chow, M. A., Leong, Y. C., Brooks, P. P., Micciche, E., Choe, G., Goldstein, A., Vanderwal, T., Halchenko, Y. O., Norman, K. A., & Hasson, U. (2021). The \"Narratives\" fMRI dataset for evaluating models of naturalistic language comprehension. *Scientific Data*, *8*, 250. https://doi.org/10.1038/s41597-021-01033-3\n",
    "\n",
    "* Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019). Voxelwise encoding models with non-spherical multivariate normal priors. *NeuroImage*, *197*, 482-492. https://doi.org/10.1016/j.neuroimage.2019.04.012\n",
    "\n",
    "* Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. *OpenAI Blog*. https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf\n",
    "\n",
    "* Schaefer, A., Kong, R., Gordon, E. M., Laumann, T. O., Zuo, X. N., Holmes, A. J., Eickhoff, S. B., & Yeo, B. T. (2018). Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI. *Cerebral Cortex*, *28*(9), 3095–3114. https://doi.org/10.1093/cercor/bhx179\n",
    "\n",
    "* Zada, Z., Goldstein, A. Y., Michelmann, S., Simony, E., Price, A., Hasenfratz, L., Barham, E., Zadbood, A., Doyle, W., Friedman, D., Dugan, P., Melloni, L., Devore, S., Flinker, A., Devinsky, O., Hasson, U.\\*, & Nastase, S. A.\\* (2023). A shared linguistic space for transmitting our thoughts from brain to brain in natural conversations. *bioRxiv*. https://doi.org/10.1101/2023.06.27.546708"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "encling",
   "language": "python",
   "name": "encling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "062ce0cefdc3424ea03131ef193071d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06c813b116dc40fbaf85c528832407fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08b47210768c4c248cbb9c74394253e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dad9254f0004d7d82f99187638d222d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "108e33f92dbe457d8487de2b2186db78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13af58c6b20649fe922bb0c6223c2f0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13f62db2e3874d668e028c9a7d8e16d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ecd3db9d03c6454b97b66c38152d4801",
       "IPY_MODEL_ffac94d3df9b4404b99904eebd430a63",
       "IPY_MODEL_64e9caad394a427bb24067f43528f259"
      ],
      "layout": "IPY_MODEL_896a1c434f6c46b6b8388837cf4a88bb",
      "tabbable": null,
      "tooltip": null
     }
    },
    "1426b1cfe4e84475b1c25d83e76eeacf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "1458bca0a9384eb4bff0ec04a9aa776b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_178ca28827c84bb0a13f9ceb680aa0d1",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6fa1483ce6014d8eb78a4ae03fa9e8e2",
      "tabbable": null,
      "tooltip": null,
      "value": 1042301
     }
    },
    "178ca28827c84bb0a13f9ceb680aa0d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "192a08da88c64b0db8f2b2f95fc7c50e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "1c8bb4a103474401bf5003513ac410ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_7ac1c8c55f0045c19d29123dc518526e",
      "placeholder": "​",
      "style": "IPY_MODEL_849ffe145e8642c09e2f07db9e40e146",
      "tabbable": null,
      "tooltip": null,
      "value": " 1.36M/1.36M [00:00&lt;00:00, 5.22MB/s]"
     }
    },
    "1d000c8ca15f4d3b9fca05712d0fc006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21318a73cce6402da95aba96df138c80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "25bb9bf7027f48be8fba443662145078": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "266d23b0dbf94c6c85b205e9cd7aba0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "280ecb113a03409d94b01e741558a3d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29dedfdd33cb48138ed8372995b281ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e30d7afdfdf4d66872eaddcf4872cf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ec8711f757f48ef88e876728099f2e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f0d1b94e5d84aa68df51ce515225552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_76f78665bd60451194a2b88a770ce22f",
       "IPY_MODEL_7b29474099f84e2ba3ea09c7982130cb",
       "IPY_MODEL_5a37e874bb3c44a29d817f73e3603c38"
      ],
      "layout": "IPY_MODEL_633fd0288ae943bdbb38b4656de761f9",
      "tabbable": null,
      "tooltip": null
     }
    },
    "2f9f20efd76d4cb4a7c2e4e445da0c92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "3591ab2035df4b9dbfa1691dfce82a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "3de6628df0644b82a3d8bb548bc267b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4046a45c3d064ef0a7892841e7f8116c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "436f762787b546ee90ed3e0b31e2cf20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_582505944b2d4f59a375eb6be475250d",
      "placeholder": "​",
      "style": "IPY_MODEL_1426b1cfe4e84475b1c25d83e76eeacf",
      "tabbable": null,
      "tooltip": null,
      "value": " 456k/456k [00:00&lt;00:00, 3.48MB/s]"
     }
    },
    "4baf36cab4bb4e53b243d52d69f27bf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af638484af0b4c5abf24f9b11dff0539",
       "IPY_MODEL_6fb88daec9824960a6b17b30a8bbd45a",
       "IPY_MODEL_436f762787b546ee90ed3e0b31e2cf20"
      ],
      "layout": "IPY_MODEL_06c813b116dc40fbaf85c528832407fc",
      "tabbable": null,
      "tooltip": null
     }
    },
    "548287f046294c3fbb9c11ad2d9dcf77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "582505944b2d4f59a375eb6be475250d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a37e874bb3c44a29d817f73e3603c38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_d0445857280f4b0a92d07ced9a049f7b",
      "placeholder": "​",
      "style": "IPY_MODEL_fc926f5508a84096ae8272528f6a8b17",
      "tabbable": null,
      "tooltip": null,
      "value": " 26.0/26.0 [00:00&lt;00:00, 1.73kB/s]"
     }
    },
    "601440f5a80f41cbac3c1c0ff73beef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "633fd0288ae943bdbb38b4656de761f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634141202d4f46edabedb85b1a83c421": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "64e9caad394a427bb24067f43528f259": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1d000c8ca15f4d3b9fca05712d0fc006",
      "placeholder": "​",
      "style": "IPY_MODEL_b8bc017558164f4181616bd99089d438",
      "tabbable": null,
      "tooltip": null,
      "value": " 153/153 [00:51&lt;00:00,  3.39it/s]"
     }
    },
    "6534373e815642788319153b42cfdb24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_72d80169b45c4f178ec667d507e94618",
      "placeholder": "​",
      "style": "IPY_MODEL_21318a73cce6402da95aba96df138c80",
      "tabbable": null,
      "tooltip": null,
      "value": "model.safetensors: 100%"
     }
    },
    "6f0c94e40b1d496c9f240f78cf3d000e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fa1483ce6014d8eb78a4ae03fa9e8e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fb88daec9824960a6b17b30a8bbd45a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_2e30d7afdfdf4d66872eaddcf4872cf4",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8179a84a39a8494db5ceff251474863a",
      "tabbable": null,
      "tooltip": null,
      "value": 456318
     }
    },
    "70b5ea4a8e8f40c1a77587af4513abb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6534373e815642788319153b42cfdb24",
       "IPY_MODEL_7788ae55e7664c029e5a930d395d99de",
       "IPY_MODEL_a41f25eb0c4d41c79940d62c0cbaf8dc"
      ],
      "layout": "IPY_MODEL_13af58c6b20649fe922bb0c6223c2f0f",
      "tabbable": null,
      "tooltip": null
     }
    },
    "72d80169b45c4f178ec667d507e94618": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76f78665bd60451194a2b88a770ce22f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_8e6fdc51719b4930bd6859cb620d7d16",
      "placeholder": "​",
      "style": "IPY_MODEL_2f9f20efd76d4cb4a7c2e4e445da0c92",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer_config.json: 100%"
     }
    },
    "7788ae55e7664c029e5a930d395d99de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_7e85b53d80c845cca020ab3383dce282",
      "max": 548105171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_828d88411bcc497a9a011151b11dfa93",
      "tabbable": null,
      "tooltip": null,
      "value": 548105171
     }
    },
    "7ac1c8c55f0045c19d29123dc518526e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b29474099f84e2ba3ea09c7982130cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_7c111bafd3ba4d8583602742cfa43f1b",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_280ecb113a03409d94b01e741558a3d9",
      "tabbable": null,
      "tooltip": null,
      "value": 26
     }
    },
    "7c111bafd3ba4d8583602742cfa43f1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7dcc21f2ee9c42c9af91a9a7c5a47f33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e85b53d80c845cca020ab3383dce282": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "802bb91c1bab4935a9e0e626e69c1c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_cbf77b1b9ca145d0b8fa200e62588b14",
      "placeholder": "​",
      "style": "IPY_MODEL_3591ab2035df4b9dbfa1691dfce82a12",
      "tabbable": null,
      "tooltip": null,
      "value": "vocab.json: 100%"
     }
    },
    "80c9606f7ad042648c265b893ff6d04f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_87f0c7da96cf46aa9cdddc2021e99fd6",
       "IPY_MODEL_8706611590594a32a2d9da8d7b3cff71",
       "IPY_MODEL_1c8bb4a103474401bf5003513ac410ef"
      ],
      "layout": "IPY_MODEL_2ec8711f757f48ef88e876728099f2e7",
      "tabbable": null,
      "tooltip": null
     }
    },
    "8179a84a39a8494db5ceff251474863a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "828d88411bcc497a9a011151b11dfa93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "846c26d4527f4be49c8479f5d20ec664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "849ffe145e8642c09e2f07db9e40e146": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "86cec5c955304fe5a04457c99f23e85c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8706611590594a32a2d9da8d7b3cff71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_108e33f92dbe457d8487de2b2186db78",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfaa3e2befab49a8b6f8ac8d315dc3a3",
      "tabbable": null,
      "tooltip": null,
      "value": 1355256
     }
    },
    "87f0c7da96cf46aa9cdddc2021e99fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_25bb9bf7027f48be8fba443662145078",
      "placeholder": "​",
      "style": "IPY_MODEL_b522502e622c437b829f57ef7cea828c",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.json: 100%"
     }
    },
    "896a1c434f6c46b6b8388837cf4a88bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c366283951a4fd39dd7faba7772d12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8e6fdc51719b4930bd6859cb620d7d16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90553677f945480b8e388d9d0ff655c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_802bb91c1bab4935a9e0e626e69c1c23",
       "IPY_MODEL_1458bca0a9384eb4bff0ec04a9aa776b",
       "IPY_MODEL_a85ef175f2614b92abe824831dacdfe7"
      ],
      "layout": "IPY_MODEL_df64d8dffa8a424dba98cb5ba221f87f",
      "tabbable": null,
      "tooltip": null
     }
    },
    "a41f25eb0c4d41c79940d62c0cbaf8dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b73dda287da74163a7beee26c22f4652",
      "placeholder": "​",
      "style": "IPY_MODEL_4046a45c3d064ef0a7892841e7f8116c",
      "tabbable": null,
      "tooltip": null,
      "value": " 548M/548M [00:03&lt;00:00, 210MB/s]"
     }
    },
    "a67d3d8cb0f445ef8712a438c15321ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "a85ef175f2614b92abe824831dacdfe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_846c26d4527f4be49c8479f5d20ec664",
      "placeholder": "​",
      "style": "IPY_MODEL_634141202d4f46edabedb85b1a83c421",
      "tabbable": null,
      "tooltip": null,
      "value": " 1.04M/1.04M [00:00&lt;00:00, 5.10MB/s]"
     }
    },
    "a95dcd67665a4a27979e5e8f2eb796a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af638484af0b4c5abf24f9b11dff0539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_86cec5c955304fe5a04457c99f23e85c",
      "placeholder": "​",
      "style": "IPY_MODEL_a67d3d8cb0f445ef8712a438c15321ea",
      "tabbable": null,
      "tooltip": null,
      "value": "merges.txt: 100%"
     }
    },
    "b18bbf6ada7441178a218103e1201da9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b19ae96fd98b42e4a81df91063c65cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b522502e622c437b829f57ef7cea828c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b73dda287da74163a7beee26c22f4652": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8bc017558164f4181616bd99089d438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b9513445421b4d99a44006434393731d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba03cc618c364a3fb06ed6acbc1b7c50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d5813bfcb5ab4912844646a5a66318c6",
       "IPY_MODEL_d5bf4fe82eb3409db366b738cd35dc65",
       "IPY_MODEL_c561e37685e442368cbc0e282d950edd"
      ],
      "layout": "IPY_MODEL_08b47210768c4c248cbb9c74394253e7",
      "tabbable": null,
      "tooltip": null
     }
    },
    "c561e37685e442368cbc0e282d950edd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_7dcc21f2ee9c42c9af91a9a7c5a47f33",
      "placeholder": "​",
      "style": "IPY_MODEL_e48421204bbd4b39ab8da5272f5b6efb",
      "tabbable": null,
      "tooltip": null,
      "value": " 124/124 [00:00&lt;00:00, 9.42kB/s]"
     }
    },
    "cbf77b1b9ca145d0b8fa200e62588b14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfaa3e2befab49a8b6f8ac8d315dc3a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0445857280f4b0a92d07ced9a049f7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1ab9c3c61e440f599b9f40df06ccf80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de48e5f069104ce0ae05f4080a1bca9a",
       "IPY_MODEL_d324ed87f5474cf989a0cd395c36f06b",
       "IPY_MODEL_ff70ced390954ddea604a36f609b7efd"
      ],
      "layout": "IPY_MODEL_29dedfdd33cb48138ed8372995b281ce",
      "tabbable": null,
      "tooltip": null
     }
    },
    "d324ed87f5474cf989a0cd395c36f06b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_548287f046294c3fbb9c11ad2d9dcf77",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f44352668642421abb168d16d3baab40",
      "tabbable": null,
      "tooltip": null,
      "value": 665
     }
    },
    "d5813bfcb5ab4912844646a5a66318c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_062ce0cefdc3424ea03131ef193071d1",
      "placeholder": "​",
      "style": "IPY_MODEL_266d23b0dbf94c6c85b205e9cd7aba0b",
      "tabbable": null,
      "tooltip": null,
      "value": "generation_config.json: 100%"
     }
    },
    "d5bf4fe82eb3409db366b738cd35dc65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_601440f5a80f41cbac3c1c0ff73beef7",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3de6628df0644b82a3d8bb548bc267b3",
      "tabbable": null,
      "tooltip": null,
      "value": 124
     }
    },
    "de48e5f069104ce0ae05f4080a1bca9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_0dad9254f0004d7d82f99187638d222d",
      "placeholder": "​",
      "style": "IPY_MODEL_b18bbf6ada7441178a218103e1201da9",
      "tabbable": null,
      "tooltip": null,
      "value": "config.json: 100%"
     }
    },
    "df64d8dffa8a424dba98cb5ba221f87f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e48421204bbd4b39ab8da5272f5b6efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "ecd3db9d03c6454b97b66c38152d4801": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a95dcd67665a4a27979e5e8f2eb796a6",
      "placeholder": "​",
      "style": "IPY_MODEL_192a08da88c64b0db8f2b2f95fc7c50e",
      "tabbable": null,
      "tooltip": null,
      "value": "100%"
     }
    },
    "f44352668642421abb168d16d3baab40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc926f5508a84096ae8272528f6a8b17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "ff70ced390954ddea604a36f609b7efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_6f0c94e40b1d496c9f240f78cf3d000e",
      "placeholder": "​",
      "style": "IPY_MODEL_b19ae96fd98b42e4a81df91063c65cbc",
      "tabbable": null,
      "tooltip": null,
      "value": " 665/665 [00:00&lt;00:00, 46.1kB/s]"
     }
    },
    "ffac94d3df9b4404b99904eebd430a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b9513445421b4d99a44006434393731d",
      "max": 153,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c366283951a4fd39dd7faba7772d12d",
      "tabbable": null,
      "tooltip": null,
      "value": 153
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
